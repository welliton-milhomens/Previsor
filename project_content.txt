File: .env
------------------------
DB_HOST=localhost
DB_PORT=5432
DB_NAME=finaiti_db
DB_USER=financeiro_user
DB_PASSWORD=@FinAiTi
API_KEY_YAHOO_FINANCE=b3a9833084msh399f6e762e8c372p14290fjsnd51c8487a06b
ALPHA_VANTAGE_API_KEY=IALU2BYPTYLX81CM

TWITTER_API_KEY = 1fpsViAXd8YX2cbxp2QBksgPO
TWITTER_API_SECRET_KEY = fjqhcpHqEPtoqZAfEJd7oEecBKnJoyeBwVWhmOHs2MHTfOrjMq
TWITTER_ACCESS_TOKEN = 1779117560803336192-4WmtlMwbLL81dAngJMyQ1kxTcIxzTK
TWITTER_ACCESS_TOKEN_SECRET = reJE22xOuBz24UDLIIoI8jQSlMDiz9qCKD3BcTblqOenK
TWITTER_BEARER_TOKEN = AAAAAAAAAAAAAAAAAAAAAELKvAEAAAAAfdNm9oOOeWHBHFRVtCQnMhXR664%3DUnp6Bol4Pj7I4ssc4odLKhDrqO0QTF0hQWrVJlXC7RJFHhFba4

File: app.py
------------------------
import sys
from flask import Flask, render_template, request, jsonify
from app.coleta_dados import coletor_sentimento
from app.coleta_dados.coletor_sentimento import ColetorSentimento
from app.modelo.treinador import TreinadorKAN
from app.coleta_dados.coletor_acoes import ColetorAcoes
from app.coleta_dados.coletor_fiis import ColetorFIIs
from app.coleta_dados.coletor_dados_economicos import ColetorDadosEconomicos
from datetime import datetime, timedelta
import numpy as np
import logging
from app.preprocessamento.normalizacao import Normalizador
from banco_dados.gerenciador_bd import GerenciadorBD
from functools import lru_cache
import os
from sqlalchemy.exc import SQLAlchemyError
from apscheduler.schedulers.background import BackgroundScheduler
from dotenv import load_dotenv
load_dotenv()

# Configura√ß√£o de logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Debug para verificar as vari√°veis de ambiente
load_dotenv()

print("Vari√°veis de ambiente:")
print(f"DB_HOST: {os.getenv('DB_HOST')}")
print(f"DB_PORT: {os.getenv('DB_PORT')}")
print(f"DB_NAME: {os.getenv('DB_NAME')}")
print(f"DB_USER: {os.getenv('DB_USER')}")
print(f"DB_PASSWORD: {'*' * len(os.getenv('DB_PASSWORD', ''))}")  # N√£o exiba a senha real

app = Flask(__name__)

coletor_sentimento = ColetorSentimento()

try:
    gerenciador_bd = GerenciadorBD()
    gerenciador_bd.criar_tabelas()
    if gerenciador_bd.testar_conexao():
        logger.info("Conex√£o com o banco de dados estabelecida com sucesso.")
    else:
        logger.error("Falha ao estabelecer conex√£o com o banco de dados.")
        sys.exit(1)
except SQLAlchemyError as e:
    logger.error(f"Erro ao conectar ao banco de dados: {str(e)}")
    sys.exit(1)

treinador = TreinadorKAN(output_dim=1, num_splines=10)
coletor_acoes = ColetorAcoes()
coletor_fiis = ColetorFIIs()
coletor_dados_economicos = ColetorDadosEconomicos()

logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@app.route('/')
def index():
    acoes = coletor_acoes.obter_lista_acoes()
    fiis = coletor_fiis.obter_lista_fiis()
    return render_template('index.html', acoes=acoes, fiis=fiis)

normalizador = Normalizador()

def inicializar_sistema():
    logger.info("Iniciando processo de atualiza√ß√£o e treinamento do sistema")
    
    # Adicionar restri√ß√µes de unicidade
    gerenciador_bd.adicionar_restricao_unicidade('acoes', ['acao', 'data'])
    gerenciador_bd.adicionar_restricao_unicidade('fiis', ['fii', 'data'])
    gerenciador_bd.adicionar_restricao_unicidade('dados_economicos', ['indicador', 'data'])
    gerenciador_bd.adicionar_restricao_unicidade('sentimento_mercado', ['ticker', 'data'])
    
    # Atualizar dados
    atualizar_dados_acoes()
    atualizar_dados_fiis()
    atualizar_dados_economicos()
    atualizar_dados_sentimento()
    
    # Treinar modelos
    treinar_modelo('acoes')
    treinar_modelo('fiis')
    treinar_modelo('dados_economicos')
    treinar_modelo('sentimento_mercado')
    
    logger.info("Processo de atualiza√ß√£o e treinamento conclu√≠do")

def atualizar_dados_acoes():
    ultima_atualizacao = gerenciador_bd.obter_ultima_data('acoes')
    if ultima_atualizacao is None or ultima_atualizacao < datetime.now().date():
        logger.info("Atualizando dados de a√ß√µes")
        coletor_acoes.atualizar_dados_acoes()

def atualizar_dados_fiis():
    ultima_atualizacao = gerenciador_bd.obter_ultima_data('fiis')
    if ultima_atualizacao is None or ultima_atualizacao < datetime.now().date():
        logger.info("Atualizando dados de FIIs")
        coletor_fiis.atualizar_dados_fiis()

def atualizar_dados_economicos():
    ultima_atualizacao = gerenciador_bd.obter_ultima_data('dados_economicos')
    if ultima_atualizacao is None or ultima_atualizacao < datetime.now().date():
        logger.info("Atualizando dados econ√¥micos")
        coletor_dados_economicos.atualizar_dados_economicos()

def atualizar_dados_sentimento():
    ultima_atualizacao = gerenciador_bd.obter_ultima_data('sentimento_mercado')
    if ultima_atualizacao is None or ultima_atualizacao < datetime.now().date():
        logger.info("Atualizando dados de sentimento")
        tickers = coletor_acoes.obter_lista_acoes() + coletor_fiis.obter_lista_fiis()
        coletor_sentimento.atualizar_sentimentos(tickers)

# Agendar a execu√ß√£o di√°ria
scheduler = BackgroundScheduler()
scheduler.add_job(func=inicializar_sistema, trigger="cron", hour=0, minute=0)
scheduler.start()

# Executar a inicializa√ß√£o imediatamente ao iniciar o aplicativo
inicializar_sistema()

def treinar_modelo(tabela):
    logger.info(f"Treinando modelo para {tabela}")
    try:
        resultado = treinador.treinar(tabela)
        if resultado is not None:
            logger.info(f"Modelo treinado com sucesso para {tabela}")
        else:
            logger.warning(f"N√£o foi poss√≠vel treinar o modelo para {tabela}")
    except Exception as e:
        logger.error(f"Erro ao treinar modelo para {tabela}: {str(e)}", exc_info=True)


@app.route('/treinar', methods=['POST'])
def treinar():
    tabela = request.form['tabela']
    try:
        if tabela not in ['acoes', 'fiis', 'dados_economicos']:
            return jsonify({'erro': 'Tabela inv√°lida'}), 400

        if not gerenciador_bd.verificar_tabela(f"{tabela}_normalizados"):
            return jsonify({'erro': f'Tabela {tabela}_normalizados n√£o existe'}), 400

        # Normalizar dados antes do treinamento
        dados_normalizados = normalizador.normalizar_dados(tabela)
        if dados_normalizados is not None and not dados_normalizados.empty:
            logger.info(f"Dados normalizados para {tabela}: {dados_normalizados.shape}")
            logger.debug(f"Colunas dos dados normalizados: {dados_normalizados.columns}")
            logger.debug(f"Primeiras linhas dos dados normalizados:\n{dados_normalizados.head()}")
            normalizador.salvar_dados_normalizados(tabela, dados_normalizados)
        else:
            logger.warning(f"N√£o foi poss√≠vel normalizar os dados para a tabela {tabela}")
            return jsonify({'erro': f'N√£o foi poss√≠vel normalizar os dados para {tabela}'}), 400

        resultado = treinador.treinar(tabela)
        if resultado is None:
            return jsonify({'mensagem': f'N√£o foi poss√≠vel treinar o modelo para {tabela} devido a dados insuficientes ou erros.'}), 400
        return jsonify({'mensagem': f'Modelo treinado e salvo com √™xito para {tabela}'})
    except Exception as e:
        logger.error(f'Erro ao treinar modelo para {tabela}: {str(e)}', exc_info=True)
        return jsonify({'erro': f'Erro ao treinar modelo: {str(e)}'}), 500

@app.route('/prever', methods=['GET'])
def prever():
    tabela = request.args.get('tabela')
    tickers = request.args.getlist('tickers')
    dias = int(request.args.get('dias', 7))
    
    logger.debug(f"Iniciando previs√£o para tabela: {tabela}, tickers: {tickers}, dias: {dias}")
    
    normalizador.carregar_scalers(tabela)
    
    # Carregar o modelo antes de fazer previs√µes
    treinador.carregar_modelo(tabela)
    
    resultados = {}
    
    try:
        for ticker in tickers:
            X, _ = treinador.preparar_dados(tabela, ticker)
            if X is None or len(X) < 30:
                logger.warning(f"Dados hist√≥ricos insuficientes para {ticker}")
                continue

            X = np.array(X)
            logger.debug(f"Forma de X para {ticker}: {X.shape}")
            
            if treinador.model is None:
                logger.error("Modelo n√£o treinado")
                return jsonify({'erro': 'Modelo n√£o treinado. Por favor, treine o modelo primeiro.'}), 400
            
            previsoes_normalizadas = treinador.prever(X[-1:], dias)
            
            previsoes_desnormalizadas = [normalizador.desnormalizar_valor(valor, 'fechamento', tabela) for valor in previsoes_normalizadas]
            
            ultimos_valores_reais = [normalizador.desnormalizar_valor(valor[3], 'fechamento', tabela) for valor in X[-7:, :, 3]]
            
            previsoes_validadas = treinador.validar_previsoes(previsoes_desnormalizadas, ultimos_valores_reais)
            
            datas_historicas = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(7, 0, -1)]
            datas_futuras = [(datetime.now() + timedelta(days=i)).strftime('%Y-%m-%d') for i in range(dias)]
            
            sentimento_medio = float(X[-1, -1, -1]) if X.shape[2] > 5 else 0.0
            
            resultados[ticker] = {
                'ultimos_valores_reais': ultimos_valores_reais,
                'datas_historicas': datas_historicas,
                'previsao': previsoes_validadas,
                'datas_futuras': datas_futuras,
                'sentimento_medio': sentimento_medio,
                'sentimento_incorporado': X.shape[2] > 5
            }
        
        logger.debug(f"Resultados: {resultados}")
        return jsonify(resultados)
    except Exception as e:
        logger.error(f'Erro ao fazer previs√£o: {str(e)}', exc_info=True)
        return jsonify({'erro': f'Erro ao fazer previs√£o: {str(e)}'}), 500
    
def obter_dados_historicos(tabela, ticker):
    logger.debug(f"Obtendo dados hist√≥ricos para tabela: {tabela}, ticker: {ticker}")
    try:
        if tabela == 'acoes':
            query = "SELECT abertura, maxima, minima, fechamento, volume FROM acoes_normalizados WHERE acao = :ticker ORDER BY data DESC LIMIT 30"
        elif tabela == 'fiis':
            query = "SELECT abertura, maxima, minima, fechamento, volume, dividendos FROM fiis_normalizados WHERE fii = :ticker ORDER BY data DESC LIMIT 30"
        else:
            raise ValueError(f"Tabela n√£o reconhecida: {tabela}")

        dados = gerenciador_bd.executar_query(query, {'ticker': ticker})
        
        if dados.empty or len(dados) < 30:
            logger.warning(f"Dados insuficientes para {ticker}")
            return None
        
        dados = dados.iloc[::-1]  # Inverter a ordem para que os dados mais recentes estejam no final
        dados = dados.astype('float32')
        
        logger.debug(f"Dados hist√≥ricos (normalizados):\n{dados.head()}")
        
        return dados.values
    except Exception as e:
        logger.error(f"Erro ao obter dados hist√≥ricos: {str(e)}", exc_info=True)
        return None
    
@app.route('/dados_historicos', methods=['GET'])
def dados_historicos():
    tabela = request.args.get('tabela')
    ticker = request.args.get('ticker')
    
    if not tabela or not ticker:
        return jsonify({'erro': 'Par√¢metros inv√°lidos'}), 400
    
    dados = obter_dados_historicos(tabela, ticker)
    if dados is None:
        return jsonify({'erro': 'Dados n√£o encontrados'}), 404
    
    return jsonify({'dados': dados.tolist()})

@app.route('/comparar', methods=['GET'])
def comparar():
    tabela = request.args.get('tabela')
    tickers = request.args.getlist('tickers')
    
    if not tabela or not tickers:
        return jsonify({'erro': 'Par√¢metros inv√°lidos'}), 400
    
    resultados = {}
    for ticker in tickers:
        dados = obter_dados_historicos(tabela, ticker)
        if dados is not None:
            resultados[ticker] = dados.tolist()
    
    return jsonify(resultados)

@app.route('/atualizar_sentimentos', methods=['POST'])
def atualizar_sentimentos():
    tickers = request.json.get('tickers', [])
    if not tickers:
        return jsonify({'erro': 'Nenhum ticker fornecido'}), 400
    
    try:
        coletor_sentimento = ColetorSentimento()
        coletor_sentimento.atualizar_sentimentos(tickers)
        return jsonify({'mensagem': 'Sentimentos atualizados com sucesso'})
    except Exception as e:
        logger.error(f'Erro ao atualizar sentimentos: {str(e)}', exc_info=True)
        return jsonify({'erro': f'Erro ao atualizar sentimentos: {str(e)}'}), 500
    

if __name__ == '__main__':
    print("Servidor iniciando...")
    print("Acesse http://127.0.0.1:5000 no seu navegador")
    app.run(debug=True, use_reloader=False)

File: requirements.txt
------------------------
absl-py==2.1.0
aiohttp==3.9.5
aiosignal==1.3.1
alabaster @ file:///home/conda/feedstock_root/build_artifacts/alabaster_1704848697227/work
alembic==1.13.1
alpha_vantage==3.0.0
annotated-types==0.7.0
anyio @ file:///home/conda/feedstock_root/build_artifacts/anyio_1717693030552/work
argon2-cffi @ file:///home/conda/feedstock_root/build_artifacts/argon2-cffi_1692818318753/work
argon2-cffi-bindings @ file:///D:/bld/argon2-cffi-bindings_1695386877556/work
arrow @ file:///home/conda/feedstock_root/build_artifacts/arrow_1696128962909/work
astroid @ file:///D:/bld/astroid_1716193889563/work
asttokens @ file:///home/conda/feedstock_root/build_artifacts/asttokens_1698341106958/work
astunparse==1.6.3
async-lru @ file:///home/conda/feedstock_root/build_artifacts/async-lru_1690563019058/work
async-timeout==4.0.3
atomicwrites @ file:///home/conda/feedstock_root/build_artifacts/atomicwrites_1657325823582/work
attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1704011227531/work
autopage==0.5.2
autopep8 @ file:///home/conda/feedstock_root/build_artifacts/autopep8_1693061251004/work
Babel @ file:///home/conda/feedstock_root/build_artifacts/babel_1702422572539/work
backports.tarfile @ file:///home/conda/feedstock_root/build_artifacts/backports.tarfile_1712700614056/work
bcrypt @ file:///D:/bld/bcrypt_1715971809558/work
beautifulsoup4 @ file:///home/conda/feedstock_root/build_artifacts/beautifulsoup4_1705564648255/work
binaryornot==0.4.4
black @ file:///D:/bld/black-recipe_1714119777660/work
bleach @ file:///home/conda/feedstock_root/build_artifacts/bleach_1696630167146/work
blinker==1.8.2
Brotli @ file:///D:/bld/brotli-split_1695989908365/work
cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work
certifi @ file:///home/conda/feedstock_root/build_artifacts/certifi_1718025014955/work/certifi
cffi @ file:///D:/bld/cffi_1696001739918/work
chardet @ file:///D:/bld/chardet_1695468773258/work
charset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1698833585322/work
click @ file:///D:/bld/click_1692312014553/work
cliff==4.7.0
cloudpickle @ file:///home/conda/feedstock_root/build_artifacts/cloudpickle_1697464713350/work
cmaes==0.10.0
cmd2==2.4.3
colorama @ file:///home/conda/feedstock_root/build_artifacts/colorama_1666700638685/work
colorlog==6.8.2
comm @ file:///home/conda/feedstock_root/build_artifacts/comm_1710320294760/work
contourpy==1.2.1
cookiecutter @ file:///home/conda/feedstock_root/build_artifacts/cookiecutter_1708608886262/work
cryptography @ file:///D:/bld/cryptography-split_1717559564603/work
cycler==0.12.1
debugpy @ file:///D:/bld/debugpy_1707444604970/work
decorator @ file:///home/conda/feedstock_root/build_artifacts/decorator_1641555617451/work
defusedxml @ file:///home/conda/feedstock_root/build_artifacts/defusedxml_1615232257335/work
diff-match-patch @ file:///home/conda/feedstock_root/build_artifacts/diff-match-patch_1683670697993/work
dill @ file:///home/conda/feedstock_root/build_artifacts/dill_1706434688412/work
dnspython==2.6.1
docstring-to-markdown @ file:///home/conda/feedstock_root/build_artifacts/docstring-to-markdown_1708563025188/work
docutils @ file:///home/conda/feedstock_root/build_artifacts/docutils_1713930378730/work
email_validator==2.1.1
entrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1643888246732/work
exceptiongroup @ file:///home/conda/feedstock_root/build_artifacts/exceptiongroup_1704921103267/work
executing @ file:///home/conda/feedstock_root/build_artifacts/executing_1698579936712/work
fastapi==0.111.0
fastapi-cli==0.0.4
fastjsonschema @ file:///home/conda/feedstock_root/build_artifacts/python-fastjsonschema_1703780968325/work/dist
filelock @ file:///home/conda/feedstock_root/build_artifacts/filelock_1714422806336/work
flake8 @ file:///home/conda/feedstock_root/build_artifacts/flake8_1704483779980/work
Flask==3.0.3
flatbuffers==24.3.25
fonttools==4.53.0
fqdn @ file:///home/conda/feedstock_root/build_artifacts/fqdn_1638810296540/work/dist
frozendict==2.4.4
frozenlist==1.4.1
fsspec==2024.6.0
future==1.0.0
gast==0.5.4
google-pasta==0.2.0
greenlet==3.0.3
grpcio==1.64.1
h11 @ file:///home/conda/feedstock_root/build_artifacts/h11_1664132893548/work
h2 @ file:///home/conda/feedstock_root/build_artifacts/h2_1634280454336/work
h5py==3.11.0
hpack==4.0.0
html5lib==1.1
httpcore @ file:///home/conda/feedstock_root/build_artifacts/httpcore_1711596990900/work
httptools==0.6.1
httpx @ file:///home/conda/feedstock_root/build_artifacts/httpx_1708530890843/work
hyperframe @ file:///home/conda/feedstock_root/build_artifacts/hyperframe_1619110129307/work
idna @ file:///home/conda/feedstock_root/build_artifacts/idna_1713279365350/work
imagesize @ file:///home/conda/feedstock_root/build_artifacts/imagesize_1656939531508/work
importlib_metadata @ file:///home/conda/feedstock_root/build_artifacts/importlib-metadata_1710971335535/work
importlib_resources @ file:///home/conda/feedstock_root/build_artifacts/importlib_resources_1711040877059/work
inflection @ file:///home/conda/feedstock_root/build_artifacts/inflection_1598089801258/work
intel-openmp==2021.4.0
intervaltree @ file:///home/conda/feedstock_root/build_artifacts/intervaltree_1683532206518/work
ipykernel @ file:///D:/bld/ipykernel_1717717683217/work
ipython @ file:///D:/bld/ipython_1701831845989/work
ipywidgets @ file:///home/conda/feedstock_root/build_artifacts/ipywidgets_1716897651763/work
isoduration @ file:///home/conda/feedstock_root/build_artifacts/isoduration_1638811571363/work/dist
isort @ file:///home/conda/feedstock_root/build_artifacts/isort_1702518492027/work
itsdangerous==2.2.0
jaraco.classes @ file:///home/conda/feedstock_root/build_artifacts/jaraco.classes_1713939306738/work
jaraco.context @ file:///home/conda/feedstock_root/build_artifacts/jaraco.context_1714372187587/work
jaraco.functools @ file:///home/conda/feedstock_root/build_artifacts/jaraco.functools_1701695162614/work
jedi @ file:///home/conda/feedstock_root/build_artifacts/jedi_1696326070614/work
jellyfish @ file:///D:/bld/jellyfish_1716993225584/work
Jinja2 @ file:///home/conda/feedstock_root/build_artifacts/jinja2_1715127149914/work
joblib==1.4.2
json5 @ file:///home/conda/feedstock_root/build_artifacts/json5_1712986206667/work
jsonpointer @ file:///D:/bld/jsonpointer_1695397475147/work
jsonschema @ file:///home/conda/feedstock_root/build_artifacts/jsonschema-meta_1714573116818/work
jsonschema-specifications @ file:///tmp/tmpkv1z7p57/src
jupyter @ file:///home/conda/feedstock_root/build_artifacts/jupyter_1696255489086/work
jupyter-console @ file:///home/conda/feedstock_root/build_artifacts/jupyter_console_1678118109161/work
jupyter-events @ file:///home/conda/feedstock_root/build_artifacts/jupyter_events_1710805637316/work
jupyter-lsp @ file:///home/conda/feedstock_root/build_artifacts/jupyter-lsp-meta_1712707420468/work/jupyter-lsp
jupyter_client @ file:///home/conda/feedstock_root/build_artifacts/jupyter_client_1716472197302/work
jupyter_core @ file:///D:/bld/jupyter_core_1710257377578/work
jupyter_server @ file:///home/conda/feedstock_root/build_artifacts/jupyter_server_1717122053158/work
jupyter_server_terminals @ file:///home/conda/feedstock_root/build_artifacts/jupyter_server_terminals_1710262634903/work
jupyterlab==4.2.1
jupyterlab_pygments @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_pygments_1707149102966/work
jupyterlab_server @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_server-split_1716433953404/work
jupyterlab_widgets @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_widgets_1716891641122/work
keras==3.3.3
keyring @ file:///D:/bld/keyring_1715715360437/work
kiwisolver==1.4.5
libclang==18.1.1
lightning==2.2.5
lightning-utilities==0.11.2
lxml==5.2.2
Mako==1.3.5
Markdown==3.6
markdown-it-py @ file:///home/conda/feedstock_root/build_artifacts/markdown-it-py_1686175045316/work
MarkupSafe @ file:///D:/bld/markupsafe_1706900063757/work
matplotlib==3.9.0
matplotlib-inline @ file:///home/conda/feedstock_root/build_artifacts/matplotlib-inline_1713250518406/work
mccabe @ file:///home/conda/feedstock_root/build_artifacts/mccabe_1643049622439/work
mdurl @ file:///home/conda/feedstock_root/build_artifacts/mdurl_1704317613764/work
mistune @ file:///home/conda/feedstock_root/build_artifacts/mistune_1698947099619/work
mkl==2021.4.0
ml-dtypes==0.3.2
more-itertools @ file:///home/conda/feedstock_root/build_artifacts/more-itertools_1704738417589/work
mpmath @ file:///home/conda/feedstock_root/build_artifacts/mpmath_1678228039184/work
multidict==6.0.5
multitasking==0.0.11
mypy-extensions @ file:///home/conda/feedstock_root/build_artifacts/mypy_extensions_1675543315189/work
namex==0.0.8
nbclient @ file:///home/conda/feedstock_root/build_artifacts/nbclient_1710317608672/work
nbconvert @ file:///home/conda/feedstock_root/build_artifacts/nbconvert-meta_1714477135335/work
nbformat @ file:///home/conda/feedstock_root/build_artifacts/nbformat_1712238998817/work
nest_asyncio @ file:///home/conda/feedstock_root/build_artifacts/nest-asyncio_1705850609492/work
networkx @ file:///home/conda/feedstock_root/build_artifacts/networkx_1698504735452/work
nltk==3.8.1
notebook @ file:///home/conda/feedstock_root/build_artifacts/notebook_1717767745914/work
notebook_shim @ file:///home/conda/feedstock_root/build_artifacts/notebook-shim_1707957777232/work
numpy @ file:///D:/bld/numpy_1707225561314/work/dist/numpy-1.26.4-cp39-cp39-win_amd64.whl#sha256=af5f40857bb7ceb2d3be562fa763863d2f954c104873e89b8ee370f75e047ecf
numpydoc @ file:///home/conda/feedstock_root/build_artifacts/numpydoc_1717502870243/work
opt-einsum==3.3.0
optree==0.11.0
optuna==3.6.1
orjson==3.10.3
overrides @ file:///home/conda/feedstock_root/build_artifacts/overrides_1706394519472/work
packaging @ file:///home/conda/feedstock_root/build_artifacts/packaging_1710075952259/work
pandas==2.2.2
pandocfilters @ file:///home/conda/feedstock_root/build_artifacts/pandocfilters_1631603243851/work
paramiko @ file:///home/conda/feedstock_root/build_artifacts/paramiko_1703015906107/work
parso @ file:///home/conda/feedstock_root/build_artifacts/parso_1712320355065/work
pathspec @ file:///home/conda/feedstock_root/build_artifacts/pathspec_1702249949303/work
patsy==0.5.6
pbr==6.0.0
peewee==3.17.5
pexpect @ file:///home/conda/feedstock_root/build_artifacts/pexpect_1706113125309/work
pickleshare @ file:///home/conda/feedstock_root/build_artifacts/pickleshare_1602536217715/work
pillow @ file:///D:/bld/pillow_1712154573692/work
pkgutil_resolve_name @ file:///home/conda/feedstock_root/build_artifacts/pkgutil-resolve-name_1694617248815/work
platformdirs @ file:///home/conda/feedstock_root/build_artifacts/platformdirs_1715777629804/work
pluggy @ file:///home/conda/feedstock_root/build_artifacts/pluggy_1713667077545/work
ply @ file:///home/conda/feedstock_root/build_artifacts/ply_1712242996588/work
prettytable==3.10.0
prometheus_client @ file:///home/conda/feedstock_root/build_artifacts/prometheus_client_1707932675456/work
prompt_toolkit @ file:///home/conda/feedstock_root/build_artifacts/prompt-toolkit_1717583537988/work
protobuf==4.25.3
psutil @ file:///D:/bld/psutil_1705722541723/work
psycopg2-binary==2.9.9
ptyprocess @ file:///home/conda/feedstock_root/build_artifacts/ptyprocess_1609419310487/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl
pure-eval @ file:///home/conda/feedstock_root/build_artifacts/pure_eval_1642875951954/work
pycodestyle @ file:///home/conda/feedstock_root/build_artifacts/pycodestyle_1697202867721/work
pycparser @ file:///home/conda/feedstock_root/build_artifacts/pycparser_1711811537435/work
pydantic==2.7.3
pydantic_core==2.18.4
pyDeprecate==0.3.1
pydocstyle @ file:///home/conda/feedstock_root/build_artifacts/pydocstyle_1673997487070/work
pyflakes @ file:///home/conda/feedstock_root/build_artifacts/pyflakes_1704424584912/work
Pygments @ file:///home/conda/feedstock_root/build_artifacts/pygments_1714846767233/work
pylint @ file:///home/conda/feedstock_root/build_artifacts/pylint_1717705591781/work
pylint-venv @ file:///home/conda/feedstock_root/build_artifacts/pylint-venv_1698219336631/work
pyls-spyder @ file:///home/conda/feedstock_root/build_artifacts/pyls-spyder_1619747398504/work
PyNaCl @ file:///D:/bld/pynacl_1695545054459/work
pyparsing==3.1.2
pyperclip==1.8.2
PyQt5==5.15.9
PyQt5-sip @ file:///D:/bld/pyqt-split_1695417932202/work/pyqt_sip
PyQtWebEngine==5.15.4
pyreadline3==3.4.1
PySocks @ file:///D:/bld/pysocks_1661604991356/work
python-dateutil @ file:///home/conda/feedstock_root/build_artifacts/python-dateutil_1709299778482/work
python-dotenv==1.0.1
python-json-logger @ file:///home/conda/feedstock_root/build_artifacts/python-json-logger_1677079630776/work
python-lsp-black @ file:///home/conda/feedstock_root/build_artifacts/python-lsp-black_1702956932456/work
python-lsp-jsonrpc @ file:///home/conda/feedstock_root/build_artifacts/python-lsp-jsonrpc_1695528365348/work
python-lsp-server @ file:///home/conda/feedstock_root/build_artifacts/python-lsp-server-meta_1711734797703/work
python-multipart==0.0.9
python-slugify @ file:///home/conda/feedstock_root/build_artifacts/python-slugify-split_1707425621764/work
pytoolconfig @ file:///home/conda/feedstock_root/build_artifacts/pytoolconfig_1675124745143/work
pytorch-forecasting==1.0.0
pytorch-lightning==2.2.5
pytorch_optimizer==2.12.0
pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1706886791323/work
pywin32==306
pywin32-ctypes @ file:///D:/bld/pywin32-ctypes_1695395076921/work
pywinpty @ file:///D:/bld/pywinpty_1708993010363/work/target/wheels/pywinpty-2.0.13-cp39-none-win_amd64.whl#sha256=84b4ae7fb10140f362ee7ce26d822343cddcefa8007d779a0392e447cd05b281
PyYAML @ file:///D:/bld/pyyaml_1695373613960/work
pyzmq @ file:///D:/bld/pyzmq_1715024536945/work
QDarkStyle @ file:///home/conda/feedstock_root/build_artifacts/qdarkstyle_1702957860620/work
qstylizer @ file:///home/conda/feedstock_root/build_artifacts/qstylizer_1713394158018/work/dist/qstylizer-0.2.3-py2.py3-none-any.whl#sha256=9f7b5d720ea5b7459972f570e09067f8286e9d00955e2b91ef2bd4d0b5edce1c
QtAwesome @ file:///home/conda/feedstock_root/build_artifacts/qtawesome_1711652164228/work
qtconsole @ file:///home/conda/feedstock_root/build_artifacts/qtconsole-base_1714942934316/work
QtPy @ file:///home/conda/feedstock_root/build_artifacts/qtpy_1698112029416/work
referencing @ file:///home/conda/feedstock_root/build_artifacts/referencing_1714619483868/work
regex==2024.7.24
requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1717057054362/work
rfc3339-validator @ file:///home/conda/feedstock_root/build_artifacts/rfc3339-validator_1638811747357/work
rfc3986-validator @ file:///home/conda/feedstock_root/build_artifacts/rfc3986-validator_1598024191506/work
rich @ file:///home/conda/feedstock_root/build_artifacts/rich-split_1709150387247/work/dist
rope @ file:///home/conda/feedstock_root/build_artifacts/rope_1711296293824/work
rpds-py @ file:///D:/bld/rpds-py_1715090106700/work
Rtree @ file:///D:/bld/rtree_1705698060554/work
scikeras==0.13.0
scikit-learn==1.5.0
scipy==1.13.1
seaborn==0.13.2
Send2Trash @ file:///D:/bld/send2trash_1712585174948/work
shellingham==1.5.4
sip @ file:///D:/bld/sip_1697300548918/work
six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work
sniffio @ file:///home/conda/feedstock_root/build_artifacts/sniffio_1708952932303/work
snowballstemmer @ file:///home/conda/feedstock_root/build_artifacts/snowballstemmer_1637143057757/work
sortedcontainers @ file:///home/conda/feedstock_root/build_artifacts/sortedcontainers_1621217038088/work
soupsieve @ file:///home/conda/feedstock_root/build_artifacts/soupsieve_1693929250441/work
Sphinx @ file:///home/conda/feedstock_root/build_artifacts/sphinx_1713554891974/work
sphinxcontrib-applehelp @ file:///home/conda/feedstock_root/build_artifacts/sphinxcontrib-applehelp_1705126298355/work
sphinxcontrib-devhelp @ file:///home/conda/feedstock_root/build_artifacts/sphinxcontrib-devhelp_1705126010477/work
sphinxcontrib-htmlhelp @ file:///home/conda/feedstock_root/build_artifacts/sphinxcontrib-htmlhelp_1705118152391/work
sphinxcontrib-jsmath @ file:///home/conda/feedstock_root/build_artifacts/sphinxcontrib-jsmath_1691604704163/work
sphinxcontrib-qthelp @ file:///home/conda/feedstock_root/build_artifacts/sphinxcontrib-qthelp_1705126152907/work
sphinxcontrib-serializinghtml @ file:///home/conda/feedstock_root/build_artifacts/sphinxcontrib-serializinghtml_1705118225549/work
spyder @ file:///D:/bld/spyder_1712781107353/work
spyder-kernels @ file:///D:/bld/spyder-kernels_1709087829200/work
SQLAlchemy==2.0.30
stack-data @ file:///home/conda/feedstock_root/build_artifacts/stack_data_1669632077133/work
starlette==0.37.2
statsmodels==0.14.2
stevedore==5.2.0
sympy @ file:///home/conda/feedstock_root/build_artifacts/sympy_1684180539862/work
tabulate @ file:///home/conda/feedstock_root/build_artifacts/tabulate_1665138452165/work
tbb==2021.12.0
tensorboard==2.16.2
tensorboard-data-server==0.7.2
tensorflow==2.16.1
tensorflow-intel==2.16.1
tensorflow-io-gcs-filesystem==0.31.0
termcolor==2.4.0
terminado @ file:///D:/bld/terminado_1710262761616/work
text-unidecode @ file:///home/conda/feedstock_root/build_artifacts/text-unidecode_1694707102786/work
textblob==0.18.0.post0
textdistance @ file:///home/conda/feedstock_root/build_artifacts/textdistance_1714078304435/work
threadpoolctl==3.5.0
three-merge @ file:///home/conda/feedstock_root/build_artifacts/three-merge_1595515817927/work
tinycss2 @ file:///home/conda/feedstock_root/build_artifacts/tinycss2_1713974937325/work
toml @ file:///home/conda/feedstock_root/build_artifacts/toml_1604308577558/work
tomli @ file:///home/conda/feedstock_root/build_artifacts/tomli_1644342247877/work
tomlkit @ file:///home/conda/feedstock_root/build_artifacts/tomlkit_1715185399719/work
torch==2.3.1
torchaudio==2.3.1
torchmetrics==1.4.0.post0
torchvision==0.18.1
tornado @ file:///D:/bld/tornado_1717722891796/work
tqdm==4.66.4
traitlets @ file:///home/conda/feedstock_root/build_artifacts/traitlets_1713535121073/work
typer==0.12.3
types-python-dateutil @ file:///home/conda/feedstock_root/build_artifacts/types-python-dateutil_1710589910274/work
typing==3.7.4.3
typing-utils @ file:///home/conda/feedstock_root/build_artifacts/typing_utils_1622899189314/work
typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1717802530399/work
tzdata==2024.1
ujson @ file:///D:/bld/ujson_1715783264372/work
uri-template @ file:///home/conda/feedstock_root/build_artifacts/uri-template_1688655812972/work/dist
urllib3 @ file:///home/conda/feedstock_root/build_artifacts/urllib3_1708239446578/work
uvicorn==0.30.1
watchdog @ file:///D:/bld/watchdog_1716561913175/work
watchfiles==0.22.0
wcwidth @ file:///home/conda/feedstock_root/build_artifacts/wcwidth_1704731205417/work
webcolors @ file:///home/conda/feedstock_root/build_artifacts/webcolors_1717667289718/work
webencodings @ file:///home/conda/feedstock_root/build_artifacts/webencodings_1694681268211/work
websocket-client @ file:///home/conda/feedstock_root/build_artifacts/websocket-client_1713923384721/work
websockets==12.0
Werkzeug==3.0.3
whatthepatch @ file:///home/conda/feedstock_root/build_artifacts/whatthepatch_1683396758362/work
widgetsnbextension @ file:///home/conda/feedstock_root/build_artifacts/widgetsnbextension_1716891659446/work
win-inet-pton @ file:///D:/bld/win_inet_pton_1667051142467/work
wrapt==1.16.0
yapf @ file:///home/conda/feedstock_root/build_artifacts/yapf_1690387939953/work
yarl==1.9.4
yfinance==0.2.40
zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1717845438511/work
select2==4.1.0-rc.0
pip install tweepy textblob

File: teste_twiter.py
------------------------
import tweepy
# Suas chaves de acesso
API_KEY = '1fpsViAXd8YX2cbxp2QBksgPO'
API_SECRET_KEY = 'fjqhcpHqEPtoqZAfEJd7oEecBKnJoyeBwVWhmOHs2MHTfOrjMq'
ACCESS_TOKEN = '1779117560803336192-4WmtlMwbLL81dAngJMyQ1kxTcIxzTK'
ACCESS_TOKEN_SECRET = 'reJE22xOuBz24UDLIIoI8jQSlMDiz9qCKD3BcTblqOenK'

# Autentica√ß√£o
auth = tweepy.OAuth1UserHandler(API_KEY, API_SECRET_KEY, ACCESS_TOKEN, ACCESS_TOKEN_SECRET)
api = tweepy.API(auth)

# Exemplo: pegar a linha do tempo do usu√°rio autenticado
tweets = api.home_timeline()
for tweet in tweets:
    print(f"{tweet.user.name} disse¬†{tweet.text}")

File: __init__.py
------------------------

File: app\__init__.py
------------------------

File: app\coleta_dados\coletor_acoes.py
------------------------
from datetime import timedelta
import os
import sys
import yfinance as yf
import pandas as pd

# Adicione o diret√≥rio raiz do projeto ao PYTHONPATH
projeto_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, projeto_dir)

from banco_dados.gerenciador_bd import GerenciadorBD

class ColetorAcoes:
    def __init__(self):
        self.gerenciador_bd = GerenciadorBD()

    def obter_lista_acoes(self):
        # Aqui voc√™ deve implementar a l√≥gica para obter a lista de todas as a√ß√µes da B3
        # Por simplicidade, vamos usar uma lista est√°tica por enquanto
        return ['PETR4.SA', 'VALE3.SA', 'ITUB4.SA']

    def coletar_dados_acoes(self, periodo='1y'):
        acoes = self.obter_lista_acoes()
        for acao in acoes:
            dados = yf.Ticker(acao).history(period=periodo)
            self.salvar_dados_acao(acao, dados)

    def salvar_dados_acao(self, acao, dados):
        dados_filtrados = []
        for index, row in dados.iterrows():
            dados_filtrados.append({
                'acao': acao,
                'data': index.date(),
                'abertura': row['Open'],
                'maxima': row['High'],
                'minima': row['Low'],
                'fechamento': row['Close'],
                'volume': row['Volume']
            })
        self.gerenciador_bd.upsert_dados('acoes', dados_filtrados, ['acao', 'data'])

    def atualizar_dados_acoes(self):
        acoes = self.obter_lista_acoes()
        for acao in acoes:
            ultima_data = self.gerenciador_bd.obter_ultima_data('acoes', acao)
            if ultima_data:
                start_date = ultima_data + timedelta(days=1)
                dados = yf.Ticker(acao).history(start=start_date)
            else:
                dados = yf.Ticker(acao).history(period="max")
            self.salvar_dados_acao(acao, dados)

    def obter_dados_historicos(self, acao, periodo='30d'):
        dados = yf.Ticker(acao).history(period=periodo)
        return dados[['Open', 'High', 'Low', 'Close', 'Volume']].rename(
            columns={'Open': 'abertura', 'High': 'maxima', 'Low': 'minima', 'Close': 'fechamento'}
        )

if __name__ == '__main__':
    coletor = ColetorAcoes()
    coletor.coletar_dados_acoes()

File: app\coleta_dados\coletor_dados_economicos.py
------------------------
from datetime import datetime, timedelta
import os
import sys
import yfinance as yf
import pandas as pd
import requests
from functools import lru_cache
from retrying import retry
import logging

logger = logging.getLogger(__name__)

# Adicione o diret√≥rio raiz do projeto ao PYTHONPATH
projeto_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, projeto_dir)

from banco_dados.gerenciador_bd import GerenciadorBD

class ColetorDadosEconomicos:
    def __init__(self):
        self.gerenciador_bd = GerenciadorBD()
        self.base_url = "https://api.bcb.gov.br/dados/serie/bcdata.sgs.{}/dados?formato=json"
        logger.info("ColetorDadosEconomicos inicializado")

    def coletar_dados_economicos(self):
        # Lista de c√≥digos de s√©ries do Banco Central
        series = {
            'pib': 4380,
            'inflacao': 433,
            'selic': 432,
            'cambio': 1
        }

        for nome, codigo in series.items():
            dados = self.obter_dados_serie(codigo)
            self.salvar_dados_economicos(nome, dados)

    @retry(stop_max_attempt_number=3, wait_fixed=2000)
    def obter_dados_serie(self, codigo, data_inicial=None):
        url = self.base_url.format(codigo)
        if data_inicial:
            url += f"&dataInicial={data_inicial.strftime('%d/%m/%Y')}"
        response = requests.get(url)
        response.raise_for_status()  # Isso levantar√° uma exce√ß√£o para status codes de erro
        return pd.DataFrame(response.json())
        
    @lru_cache(maxsize=32)
    def obter_dados_serie_cache(self, codigo, dias=30):
        data_inicio = datetime.now() - timedelta(days=dias)
        url = f"{self.base_url.format(codigo)}&dataInicial={data_inicio.strftime('%d/%m/%Y')}"
        return self.obter_dados_serie(codigo, url)

    def salvar_dados_economicos(self, nome, dados):
        if not dados.empty:
            dados['indicador'] = nome
            dados.rename(columns={'data': 'data', 'valor': 'valor'}, inplace=True)
            self.gerenciador_bd.upsert_dados('dados_economicos', dados.to_dict('records'), ['indicador', 'data'])
            logger.info(f"Dados econ√¥micos para {nome} atualizados com sucesso")
   
    def atualizar_dados_economicos(self):
        series = {
            'pib': 4380,
            'inflacao': 433,
            'selic': 432,
            'cambio': 1
        }

        for nome, codigo in series.items():
            ultima_data = self.gerenciador_bd.obter_ultima_data('dados_economicos', nome)
            if ultima_data:
                data_inicial = ultima_data + timedelta(days=1)
                dados = self.obter_dados_serie(codigo, data_inicial)
            else:
                dados = self.obter_dados_serie(codigo)
            
            if not dados.empty:
                self.salvar_dados_economicos(nome, dados)

if __name__ == '__main__':
    coletor = ColetorDadosEconomicos()
    coletor.coletar_dados_economicos()

File: app\coleta_dados\coletor_fiis.py
------------------------
import os
import sys
import yfinance as yf
import pandas as pd

# Adicione o diret√≥rio raiz do projeto ao PYTHONPATH
projeto_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, projeto_dir)

from banco_dados.gerenciador_bd import GerenciadorBD

class ColetorFIIs:
    def __init__(self):
        self.gerenciador_bd = GerenciadorBD()

    def obter_lista_fiis(self):
        # Aqui voc√™ deve implementar a l√≥gica para obter a lista de todos os FIIs da B3
        # Por simplicidade, vamos usar uma lista est√°tica por enquanto
        return ['HGLG11.SA', 'KNRI11.SA', 'MXRF11.SA']

    def coletar_dados_fiis(self, periodo='1y'):
        fiis = self.obter_lista_fiis()
        for fii in fiis:
            dados = yf.Ticker(fii).history(period=periodo)
            self.salvar_dados_fii(fii, dados)

    def salvar_dados_fii(self, fii, dados):
        dados['fii'] = fii
        dados.reset_index(inplace=True)
        dados.rename(columns={
            'Date': 'data',
            'Open': 'abertura',
            'High': 'maxima',
            'Low': 'minima',
            'Close': 'fechamento',
            'Volume': 'volume',
            'Dividends': 'dividendos'
        }, inplace=True)
        
        colunas_necessarias = ['fii', 'data', 'abertura', 'maxima', 'minima', 'fechamento', 'volume', 'dividendos']
        for coluna in colunas_necessarias:
            if coluna not in dados.columns:
                dados[coluna] = 0
        
        dados = dados[colunas_necessarias]
        
        self.gerenciador_bd.upsert_dados('fiis', dados.to_dict('records'), ['fii', 'data'])

    def atualizar_dados_fiis(self, periodo='1d'):
        fiis = self.obter_lista_fiis()
        for fii in fiis:
            ultima_data = self.gerenciador_bd.executar_query(
                "SELECT MAX(data) FROM fiis WHERE fii = :fii",
                {'fii': fii}
            ).iloc[0][0]
            if ultima_data:
                start_date = ultima_data + pd.Timedelta(days=1)
                dados = yf.Ticker(fii).history(start=start_date)
                self.salvar_dados_fii(fii, dados)

if __name__ == '__main__':
    coletor = ColetorFIIs()
    coletor.coletar_dados_fiis()

File: app\coleta_dados\coletor_sentimento.py
------------------------
import os
import sys
from venv import logger
import pandas as pd
from datetime import datetime, timedelta
import tweepy
from dotenv import load_dotenv
from textblob import TextBlob
import logging

logger = logging.getLogger(__name__)

# Adicione o diret√≥rio raiz do projeto ao PYTHONPATH
projeto_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, projeto_dir)

from banco_dados.gerenciador_bd import GerenciadorBD
from app.preprocessamento.analisador_sentimento import AnalisadorSentimento, mapear_sentimento

load_dotenv()

class ColetorSentimento:
    def __init__(self):
        self.client = tweepy.Client(
            bearer_token=os.getenv('TWITTER_BEARER_TOKEN'),
            consumer_key=os.getenv('TWITTER_API_KEY'),
            consumer_secret=os.getenv('TWITTER_API_SECRET_KEY'),
            access_token=os.getenv('TWITTER_ACCESS_TOKEN'),
            access_token_secret=os.getenv('TWITTER_ACCESS_TOKEN_SECRET')
        )
        self.gerenciador_bd = GerenciadorBD()
        self.analisador = AnalisadorSentimento()

    def atualizar_sentimentos(self, tickers):
        logger.info("Iniciando atualiza√ß√£o de sentimentos")
        for ticker in tickers:
            ultima_data = self.gerenciador_bd.obter_ultima_data('sentimento_mercado', ticker)
            if ultima_data is None or ultima_data < datetime.now().date():
                sentimentos = self.coletar_e_analisar_tweets(ticker)
                self.salvar_sentimentos(sentimentos)
        logger.info("Atualiza√ß√£o de sentimentos conclu√≠da")

    def coletar_tweets(self, ticker, dias=7):
        query = f"${ticker} -is:retweet lang:pt"
        data_limite = datetime.now() - timedelta(days=dias)
        tweets = []

        try:
            for tweet in tweepy.Paginator(self.client.search_recent_tweets, 
                                          query=query, 
                                          max_results=100,
                                          tweet_fields=['created_at', 'text']).flatten(limit=1000):
                if tweet.created_at < data_limite:
                    break
                tweets.append({'data': tweet.created_at, 'texto': tweet.text})

            return pd.DataFrame(tweets)
        except Exception as e:
            logger.error(f"Erro ao coletar tweets para {ticker}: {str(e)}")
            return pd.DataFrame(columns=['data', 'texto'])

    def coletar_e_analisar_tweets(self, ticker, dias=7):
        tweets_df = self.coletar_tweets(ticker, dias)
        sentimentos = []
        for _, tweet in tweets_df.iterrows():
            probabilidades = self.analisador.analisar(tweet['texto'])
            sentimento = mapear_sentimento(probabilidades)
            sentimentos.append({
                'data': tweet['data'].date(),
                'ticker': ticker,
                'sentimento': sentimento
            })
        return sentimentos
    
    def coletar_e_analisar_sentimento(self, ticker, quantidade=100):
        tweets = self.coletar_tweets(ticker, quantidade)
        sentimentos = []
        for tweet in tweets:
            sentimento = self.analisar_sentimento(tweet)
            sentimentos.append({
                'ticker': ticker,
                'data': datetime.now().date(),
                'sentimento': sentimento
            })
        return sentimentos
        
    def salvar_sentimentos(self, sentimentos):
        if not sentimentos:
            logger.warning("Nenhum sentimento para salvar")
            return

        try:
            self.gerenciador_bd.upsert_dados('sentimento_mercado', sentimentos, ['data', 'ticker'])
            logger.info(f"Sentimentos salvos com sucesso: {len(sentimentos)} registros")
        except Exception as e:
            logger.error(f"Erro ao salvar sentimentos: {str(e)}")


    def analisar_sentimento(self, texto):
        analise = TextBlob(texto)
        return analise.sentiment.polarity

    def inserir_dados_exemplo(self):
        dados_exemplo = [
            {'data': '2023-08-01', 'ticker': 'PETR4.SA', 'sentimento': 0.5},
            {'data': '2023-08-02', 'ticker': 'PETR4.SA', 'sentimento': 0.3},
            {'data': '2023-08-03', 'ticker': 'PETR4.SA', 'sentimento': 0.7},
            {'data': '2023-08-01', 'ticker': 'VALE3.SA', 'sentimento': 0.6},
            {'data': '2023-08-02', 'ticker': 'VALE3.SA', 'sentimento': 0.4},
            {'data': '2023-08-03', 'ticker': 'VALE3.SA', 'sentimento': 0.8},
            {'data': '2023-08-01', 'ticker': 'ITUB4.SA', 'sentimento': 0.2},
            {'data': '2023-08-02', 'ticker': 'ITUB4.SA', 'sentimento': 0.5},
            {'data': '2023-08-03', 'ticker': 'ITUB4.SA', 'sentimento': 0.6}
        ]
        self.salvar_sentimentos(dados_exemplo)

if __name__ == '__main__':
    coletor = ColetorSentimento()
    coletor.inserir_dados_exemplo()
    
    # Verificar os dados inseridos
    query = "SELECT * FROM sentimento_mercado ORDER BY data, ticker"
    resultado = coletor.gerenciador_bd.executar_query(query)
    print("Dados inseridos na tabela sentimento_mercado:")
    print(resultado)

File: app\coleta_dados\__init__.py
------------------------

File: app\coleta_dados\__pycache__\coletor_acoes.cpython-39.pyc
------------------------
a

    £LØf!	  „                	   @   sí   d dl mZ d dlZd dlZd dlZd dlZej†	ej†	ej†	ej†
e°°°°Zej†
d e° d dlmZ G ddÑ dÉZedkréeÉ Ze†°  dS )È    )⁄	timedeltaN)⁄
GerenciadorBDc                   @   s@   e Zd ZddÑ ZddÑ ZdddÑZdd	Ñ Zd
dÑ Zdd
dÑZdS )⁄ColetorAcoesc                 C   s   t É | _d S )N)r   ⁄gerenciador_bd©⁄self© r   ˙LC:\Users\welli\OneDrive\Documentos\finaiti\app\coleta_dados\coletor_acoes.py⁄__init__   s    zColetorAcoes.__init__c                 C   s   g d¢S )N)zPETR4.SAzVALE3.SAzITUB4.SAr   r   r   r   r	   ⁄obter_lista_acoes   s    zColetorAcoes.obter_lista_acoes⁄1yc                 C   s4   | † ° }|D ]"}t†|°j|dç}| †||° qd S )N©⁄period)r   ⁄yf⁄Ticker⁄history⁄salvar_dados_acao)r   ⁄periodo⁄acoes⁄acao⁄dadosr   r   r	   ⁄coletar_dados_acoes   s    z ColetorAcoes.coletar_dados_acoesc                 C   s`   g }|† ° D ]:\}}|†||†° |d |d |d |d |d dú° q| j†d|dd	g° d S )
N⁄Open⁄High⁄Low⁄Close⁄Volume)r   ⁄data⁄abertura⁄maxima⁄minima⁄
fechamento⁄volumer   r   r   )⁄iterrows⁄append⁄dater   ⁄upsert_dados)r   r   r   Zdados_filtrados⁄index⁄rowr   r   r	   r      s    ˘
	zColetorAcoes.salvar_dados_acaoc                 C   sh   | † ° }|D ]V}| j†d|°}|rD|tddç }t†|°j|dç}nt†|°jddç}| †||° qd S )Nr   È   )⁄days)⁄start⁄maxr
   )r   r   ⁄obter_ultima_datar   r   r   r   r   )r   r   r   ⁄ultima_data⁄
start_dater   r   r   r	   ⁄atualizar_dados_acoes*   s    z"ColetorAcoes.atualizar_dados_acoes⁄30dc                 C   s0   t †|°j|dç}|g d¢ jdddddúdçS )	Nr
   )r   r   r   r   r   r   r   r    r!   )r   r   r   r   )⁄columns)r   r   r   ⁄rename)r   r   r   r   r   r   r	   ⁄obter_dados_historicos5   s    ˇz#ColetorAcoes.obter_dados_historicosN)r   )r1   )	⁄__name__⁄
__module__⁄__qualname__r
   r   r   r   r0   r4   r   r   r   r	   r   
   s   
r   ⁄__main__)⁄datetimer   ⁄os⁄sysZyfinancer   ⁄pandas⁄pd⁄path⁄dirname⁄abspath⁄__file__⁄projeto_dir⁄insert⁄banco_dados.gerenciador_bdr   r   r5   ⁄coletorr   r   r   r   r	   ⁄<module>   s   $.

File: app\coleta_dados\__pycache__\coletor_dados_economicos.cpython-39.pyc
------------------------
a

    bKØfı  „                	   @   s»   d dl m Z mZ d dlZd dlZd dlZd dlZd dlZd dl	m
Z
 d dlmZ d dl
Z
e
†e°Zej†ej†ej†ej†e°°°°Zej†d e° d dlmZ G ddÑ dÉZedkrƒeÉ Ze†°  dS )	È    )⁄datetime⁄	timedeltaN)⁄	lru_cache)⁄retry)⁄
GerenciadorBDc                   @   sV   e Zd ZddÑ ZddÑ Zedddçdd	d
ÑÉZeddçdddÑÉZddÑ Z	ddÑ Z
dS )⁄ColetorDadosEconomicosc                 C   s   t É | _d| _t†d° d S )NzChttps://api.bcb.gov.br/dados/serie/bcdata.sgs.{}/dados?formato=jsonz#ColetorDadosEconomicos inicializado)r   ⁄gerenciador_bd⁄base_url⁄logger⁄info)⁄self© r
   ˙WC:\Users\welli\OneDrive\Documentos\finaiti\app\coleta_dados\coletor_dados_economicos.py⁄__init__   s    zColetorDadosEconomicos.__init__c                 C   s:   dddddú}|† ° D ]\}}| †|°}| †||° qd S )NÈ  È±  È∞  È   ©ZpibZinflacaoZselicZcambio)⁄items⁄obter_dados_serie⁄salvar_dados_economicos)r   ⁄series⁄nome⁄codigo⁄dadosr
   r
   r   ⁄coletar_dados_economicos   s    ¸
z/ColetorDadosEconomicos.coletar_dados_economicosÈ   i–  )Zstop_max_attempt_numberZ
wait_fixedNc                 C   sD   | j †|°}|r$|d|†d°õ ù7 }t†|°}|†°  t†|†° °S )N˙
&dataInicial=˙%d/%m/%Y)	r	   ⁄format⁄strftime⁄requests⁄get⁄raise_for_status⁄pd⁄	DataFrame⁄json)r   r   ⁄data_inicial⁄url⁄responser
   r
   r   r   &   s    
z(ColetorDadosEconomicos.obter_dados_serieÈ    )⁄maxsizeÈ   c                 C   s:   t †° t|dç }| j†|°õ d|†d°õ ù}| †||°S )N©⁄daysr   r   )r   ⁄nowr   r	   r    r!   r   )r   r   ⁄diasZdata_inicior)   r
   r
   r   ⁄obter_dados_serie_cache/   s    z.ColetorDadosEconomicos.obter_dados_serie_cachec                 C   sR   |j sN||d< |jdddúddç | j†d|†d°ddg° t†d	|õ d
ù° d S )N⁄	indicador⁄data⁄valor)r4   r5   T)⁄columns⁄inplace⁄dados_economicos⁄recordsu   Dados econ√¥micos para z atualizados com sucesso)⁄empty⁄renamer   ⁄upsert_dados⁄to_dictr
   r   )r   r   r   r
   r
   r   r   5   s
    z.ColetorDadosEconomicos.salvar_dados_economicosc                 C   sn   dddddú}|† ° D ]R\}}| j†d|°}|rL|tddç }| †||°}n
| †|°}|js| †||° qd S )Nr   r   r   r   r   r8   r.   )r   r   ⁄obter_ultima_datar   r   r:   r   )r   r   r   r   ⁄ultima_datar(   r   r
   r
   r   ⁄atualizar_dados_economicos<   s    ¸
z1ColetorDadosEconomicos.atualizar_dados_economicos)N)r-   )⁄__name__⁄
__module__⁄__qualname__r   r   r   r   r   r2   r   r@   r
   r
   r
   r   r      s   
r   ⁄__main__)r   r   ⁄os⁄sys⁄yfinance⁄yf⁄pandasr%   r"   ⁄	functoolsr   Zretryingr   ⁄logging⁄	getLoggerrA   r
   ⁄path⁄dirname⁄abspath⁄__file__⁄projeto_dir⁄insert⁄banco_dados.gerenciador_bdr   r   ⁄coletorr   r
   r
   r
   r   ⁄<module>   s    
$<

File: app\coleta_dados\__pycache__\coletor_fiis.cpython-39.pyc
------------------------
a

    -8Øf˜  „                	   @   sÜ   d dl Z d dlZd dlZd dlZe j†e j†e j†e j†e	°°°°Z
ej†d e
° d dlm
Z
 G ddÑ dÉZedkrÇeÉ Ze†°  dS )È    N)⁄
GerenciadorBDc                   @   s8   e Zd ZddÑ ZddÑ ZdddÑZdd	Ñ ZdddÑZd
S )⁄ColetorFIIsc                 C   s   t É | _d S )N)r   ⁄gerenciador_bd©⁄self© r   ˙KC:\Users\welli\OneDrive\Documentos\finaiti\app\coleta_dados\coletor_fiis.py⁄__init__
   s    zColetorFIIs.__init__c                 C   s   g d¢S )N)z	HGLG11.SAz	KNRI11.SAz	MXRF11.SAr   r   r   r   r   ⁄obter_lista_fiis   s    zColetorFIIs.obter_lista_fiis⁄1yc                 C   s4   | † ° }|D ]"}t†|°j|dç}| †||° qd S )N)⁄period)r
   ⁄yf⁄Ticker⁄history⁄salvar_dados_fii)r   ⁄periodo⁄fiis⁄fii⁄dadosr   r   r   ⁄coletar_dados_fiis   s    zColetorFIIs.coletar_dados_fiisc              	   C   s|   ||d< |j ddç |jdddddd	d
dúddç g d
¢}|D ]}||jvr>d||< q>|| }| j†d|†d°ddg° d S )Nr   T)⁄inplace⁄data⁄abertura⁄maxima⁄minima⁄
fechamento⁄volume⁄
dividendos)⁄Date⁄Open⁄High⁄Low⁄Close⁄Volume⁄	Dividends)⁄columnsr   )r   r   r   r   r   r   r   r   r   r   ⁄records)⁄reset_index⁄renamer%   r   ⁄upsert_dados⁄to_dict)r   r   r   Zcolunas_necessarias⁄colunar   r   r   r      s&    ˘¯


zColetorFIIs.salvar_dados_fii⁄1dc                 C   sd   | † ° }|D ]R}| j†dd|i°jd d }|r|tjddç }t†|°j|dç}| †	||° qd S )Nz+SELECT MAX(data) FROM fiis WHERE fii = :fiir   r   È   )⁄days)⁄start)
r
   r   ⁄executar_query⁄iloc⁄pd⁄	Timedeltar
   r   r   r   )r   r   r   r   ⁄ultima_data⁄
start_dater   r   r   r   ⁄atualizar_dados_fiis1   s    ˛˝˝z ColetorFIIs.atualizar_dados_fiisN)r   )r,   )⁄__name__⁄
__module__⁄__qualname__r	   r
   r   r   r6   r   r   r   r   r      s
   
r   ⁄__main__)⁄os⁄sys⁄yfinancer
   ⁄pandasr2   ⁄path⁄dirname⁄abspath⁄__file__⁄projeto_dir⁄insert⁄banco_dados.gerenciador_bdr   r   r7   ⁄coletorr   r   r   r   r   ⁄<module>   s   $1

File: app\coleta_dados\__pycache__\coletor_sentimento.cpython-39.pyc
------------------------
a

    ¥RØfr  „                	   @   s  d dl Z d dlZd dlmZ d dlZd dlmZmZ d dlZd dl	m
Z
 d dlmZ d dl
Z
e
†e°Ze j†e j†e j†e j†e°°°°Zej†d e° d dlmZ d dlmZmZ e
É  G dd	Ñ d	ÉZed
kr˛eÉ Ze†°  dZej† e°Z!e"dÉ e"e!É dS )
È    N)⁄logger)⁄datetime⁄	timedelta)⁄load_dotenv)⁄TextBlob)⁄
GerenciadorBD)⁄AnalisadorSentimento⁄mapear_sentimentoc                   @   sR   e Zd ZddÑ ZddÑ ZdddÑZddd	ÑZdddÑZd
dÑ ZddÑ Z	ddÑ Z
dS )⁄ColetorSentimentoc                 C   sH   t jt†d°t†d°t†d°t†d°t†d°dç| _tÉ | _tÉ | _d S )NZTWITTER_BEARER_TOKENZTWITTER_API_KEYZTWITTER_API_SECRET_KEYZTWITTER_ACCESS_TOKENZTWITTER_ACCESS_TOKEN_SECRET)Zbearer_tokenZconsumer_keyZconsumer_secretZaccess_tokenZaccess_token_secret)	⁄tweepy⁄Client⁄os⁄getenv⁄clientr   ⁄gerenciador_bdr   ⁄
analisador)⁄self© r   ˙QC:\Users\welli\OneDrive\Documentos\finaiti\app\coleta_dados\coletor_sentimento.py⁄__init__   s    ˚zColetorSentimento.__init__c                 C   s\   t †d° |D ]>}| j†d|°}|d u s8|t†° †° k r| †|°}| †|° qt †d° d S )Nu&   Iniciando atualiza√ß√£o de sentimentos⁄sentimento_mercadou'   Atualiza√ß√£o de sentimentos conclu√≠da)	r   ⁄infor   ⁄obter_ultima_datar   ⁄now⁄date⁄coletar_e_analisar_tweets⁄salvar_sentimentos)r   ⁄tickers⁄tickerZultima_data⁄sentimentosr   r   r   ⁄atualizar_sentimentos"   s    

z'ColetorSentimento.atualizar_sentimentosÈ   c              
   C   sÃ   d|õ dù}t †° t|dç }g }zVtj| jj|dddgdçjdd	çD ]&}|j|k rX qn|†	|j|j
d
ú° qFt†|°W S  t
y∆ } z6t†d|õ dt|Éõ ù° tjd
dgdçW  Y d }~S d }~0 0 d S )N˙$z -is:retweet lang:pt)⁄daysÈd   ⁄
created_at⁄text)⁄queryZmax_resultsZtweet_fieldsiË  )⁄limit)⁄data⁄textozErro ao coletar tweets para z: r)   r*   )⁄columns)r   r   r   r   Z	Paginatorr   Zsearch_recent_tweets⁄flattenr%   ⁄appendr&   ⁄pdZ	DataFrame⁄	Exceptionr   ⁄error⁄str)r   r   ⁄diasr'   Zdata_limite⁄tweets⁄tweet⁄er   r   r   ⁄coletar_tweets+   s$    
˝˝

z ColetorSentimento.coletar_tweetsc           	      C   sX   | † ||°}g }|†° D ]:\}}| j†|d °}t|É}|†|d †° ||dú° q|S )Nr*   r)   ©r)   r   ⁄
sentimento)r6   Ziterrowsr   Zanalisarr	   r-   r   )	r   r   r2   Z	tweets_dfr   ⁄_r4   Zprobabilidadesr8   r   r   r   r   >   s    
˝
z+ColetorSentimento.coletar_e_analisar_tweetsr$   c                 C   sB   | † ||°}g }|D ](}| †|°}|†|t†° †° |dú° q|S )N)r   r)   r8   )r6   ⁄analisar_sentimentor-   r   r   r   )r   r   Z
quantidader3   r   r4   r8   r   r   r   ⁄coletar_e_analisar_sentimentoK   s    

˝
z/ColetorSentimento.coletar_e_analisar_sentimentoc              
   C   s~   |st †d° d S z.| j†d|ddg° t †dt|Éõ dù° W n8 tyx } z t †dt|Éõ ù° W Y d }~n
d }~0 0 d S )NzNenhum sentimento para salvarr   r)   r   z Sentimentos salvos com sucesso: z
 registroszErro ao salvar sentimentos: )	r   ⁄warningr   Zupsert_dadosr   ⁄lenr/   r0   r1   )r   r   r5   r   r   r   r   W   s    
z$ColetorSentimento.salvar_sentimentosc                 C   s   t |É}|jjS )N)r   Z	sentimentZpolarity)r   r*   Zanaliser   r   r   r:   c   s    z%ColetorSentimento.analisar_sentimentoc                 C   sl   ddddúddddúddddúdd	d
dúdd	ddúdd	ddúdd
ddúdd
ddúdd
d
dúg	}| † |° d S )Nz
2023-08-01zPETR4.SAg      ‡?r7   z
2023-08-02g333333”?z
2023-08-03gffffffÊ?zVALE3.SAg333333„?göôôôôôŸ?göôôôôôÈ?zITUB4.SAgöôôôôô…?)r   )r   Z
dados_exemplor   r   r   ⁄inserir_dados_exemplog   s    








˜z'ColetorSentimento.inserir_dados_exemploN)r!   )r!   )r$   )⁄__name__⁄
__module__⁄__qualname__r   r    r6   r   r;   r   r:   r>   r   r   r   r   r
      s   	


r
   ⁄__main__z6SELECT * FROM sentimento_mercado ORDER BY data, tickerz-Dados inseridos na tabela sentimento_mercado:)#r
   ⁄sys⁄venvr   Zpandasr.   r   r   r   ⁄dotenvr   Ztextblobr   ⁄logging⁄	getLoggerr?   ⁄path⁄dirname⁄abspath⁄__file__Zprojeto_dir⁄insert⁄banco_dados.gerenciador_bdr   Z*app.preprocessamento.analisador_sentimentor   r	   r
   Zcoletorr>   r'   r   ⁄executar_query⁄	resultado⁄printr   r   r   r   ⁄<module>   s,   
$_

File: app\coleta_dados\__pycache__\__init__.cpython-39.pyc
------------------------
a

    ]H¨f    „                   @   s   d S )N© r   r   r   ˙GC:\Users\welli\OneDrive\Documentos\finaiti\app\coleta_dados\__init__.py⁄<module>   Û    

File: app\modelo\kan.py
------------------------
import tensorflow as tf

class KAN(tf.keras.Model):
    def __init__(self, input_dim, output_dim, num_splines=10):
        super(KAN, self).__init__()
        self.input_layer = tf.keras.layers.Input(shape=(None, input_dim))
        self.lstm_layer = tf.keras.layers.LSTM(num_splines, return_sequences=True)
        self.spline_activations = [SplineActivation() for _ in range(num_splines)]
        self.output_layer = tf.keras.layers.Dense(output_dim)

    def call(self, inputs):
        x = self.lstm_layer(inputs)
        x = tf.stack([activation(x[:, :, i]) for i, activation in enumerate(self.spline_activations)], axis=-1)
        return self.output_layer(x[:, -1, :])  # Use apenas o √∫ltimo passo de tempo

    def build(self, input_shape):
        super(KAN, self).build(input_shape)
        self.built = True
    
class SplineActivation(tf.keras.layers.Layer):
    def __init__(self, num_knots=5, order=3):
        super().__init__()
        self.num_knots = num_knots
        self.order = order

    def build(self, input_shape):
        self.knots = self.add_weight(
            name="knots",
            shape=(self.num_knots,),
            initializer="random_normal",
            trainable=True
        )
        self.coeffs = self.add_weight(
            name="coeffs",
            shape=(self.num_knots + self.order - 1,),
            initializer="random_normal",
            trainable=True
        )

    def call(self, inputs):
        return self.evaluate_spline(inputs)

    def evaluate_spline(self, x):
        # Implementa√ß√£o simplificada da avalia√ß√£o do spline
        y = tf.zeros_like(x)
        for i in range(self.num_knots):
            mask = tf.cast(x > self.knots[i], tf.float32)
            y += self.coeffs[i] * mask * tf.pow(x - self.knots[i], self.order)
        return y

File: app\modelo\treinador.py
------------------------
import os
import sys
from venv import logger

# Adiciona o diret√≥rio raiz do projeto ao sys.path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.insert(0, project_root)

import sqlalchemy
import tensorflow as tf
import numpy as np
import pandas as pd
from keras.callbacks import EarlyStopping
from banco_dados.gerenciador_bd import GerenciadorBD
from app.modelo.kan import KAN, SplineActivation
from app.coleta_dados.coletor_sentimento import ColetorSentimento
from app.preprocessamento.analisador_sentimento import AnalisadorSentimento

class TreinadorKAN:
    def __init__(self, output_dim, num_splines=10):
        self.output_dim = output_dim
        self.num_splines = num_splines
        self.model = None
        self.optimizer = tf.keras.optimizers.Adam()
        self.loss_fn = tf.keras.losses.MeanSquaredError()
        self.gerenciador_bd = GerenciadorBD()
        self.input_dim = None
        self.coletor_sentimento = ColetorSentimento()
        self.analisador_sentimento = AnalisadorSentimento()

    def criar_modelo(self, input_dim):
        logger.debug(f"Criando modelo com input_dim: {input_dim}")
        self.input_dim = input_dim
        self.model = KAN(input_dim, self.output_dim, self.num_splines)
        dummy_input = tf.keras.Input(shape=(None, input_dim) if input_dim > 1 else (None, 1))
        self.model(dummy_input)  # This will build the model
        self.model.compile(optimizer=self.optimizer, loss=self.loss_fn)

    def preparar_dados(self, tabela, ticker=None, janela_tempo=30, horizonte_previsao=1):
        if tabela == 'dados_economicos':
            return self.preparar_dados_economicos(tabela, janela_tempo, horizonte_previsao)
        
        try:
            query = f"""
                SELECT DISTINCT ON (n.data) n.data, n.abertura, n.maxima, n.minima, n.fechamento, n.volume, COALESCE(s.sentimento, 0) as sentimento
                FROM {tabela}_normalizados n
                LEFT JOIN sentimento_mercado s ON n.data = s.data AND n.{'acao' if tabela == 'acoes' else 'fii'} = s.ticker
                WHERE n.{'acao' if tabela == 'acoes' else 'fii'} = :ticker
                ORDER BY n.data DESC
                LIMIT 1000
            """
            dados = self.gerenciador_bd.executar_query(query, {'ticker': ticker})
            
            if dados.empty:
                logger.warning(f"Nenhum dado encontrado para {ticker}")
                return None, None

            # Remover colunas desnecess√°rias
            colunas_para_remover = ['id', 'acao', 'fii']
            dados = dados.drop(columns=[col for col in colunas_para_remover if col in dados.columns])
            
            # Converter todas as colunas para float32, exceto 'data'
            for col in dados.columns:
                if col != 'data':
                    dados[col] = dados[col].astype('float32')
            
            # Remover linhas com valores nulos
            dados = dados.dropna()
            
            logger.debug(f"Preparando dados para {ticker}: {dados.shape}")
            logger.debug(f"Colunas dos dados: {dados.columns}")
            logger.debug(f"Primeiras linhas dos dados:\n{dados.head()}")
            
            X, y = [], []
            for i in range(len(dados) - janela_tempo - horizonte_previsao + 1):
                X.append(dados.iloc[i:i+janela_tempo].drop('data', axis=1).values)
                y.append(dados.iloc[i+janela_tempo+horizonte_previsao-1]['fechamento'])
            
            return np.array(X), np.array(y)
        
        except Exception as e:
            logger.error(f"Erro ao preparar dados para {ticker}: {str(e)}", exc_info=True)
            return None, None

    def salvar_modelo(self, nome_modelo):
        try:
            modelo_serializado = self.model.to_json()
            pesos = self.model.get_weights()
            self.gerenciador_bd.salvar_modelo(nome_modelo, modelo_serializado, pesos)
            logger.info(f"Modelo {nome_modelo} salvo com sucesso no banco de dados")
        except sqlalchemy.exc.ProgrammingError as e:
            if "n√£o existe a rela√ß√£o" in str(e):
                logger.warning("Tabela modelos_treinados n√£o existe. Criando...")
                self.gerenciador_bd.criar_tabela_modelos()
                self.gerenciador_bd.salvar_modelo(nome_modelo, modelo_serializado, pesos)
                logger.info(f"Modelo {nome_modelo} salvo com sucesso ap√≥s criar a tabela")
            else:
                raise

    def carregar_modelo(self, tabela):
        nome_modelo = f"modelo_{tabela}"
        try:
            modelo_json, pesos = self.gerenciador_bd.carregar_modelo(nome_modelo)
            if modelo_json and pesos:
                self.model = tf.keras.models.model_from_json(modelo_json, custom_objects={'KAN': KAN, 'SplineActivation': SplineActivation})
                self.model.set_weights(pesos)
                logger.info(f"Modelo para {tabela} carregado com sucesso")
            else:
                logger.warning(f"Nenhum modelo encontrado para {tabela}")
        except Exception as e:
            logger.error(f"Erro ao carregar modelo para {tabela}: {str(e)}", exc_info=True)

    def preparar_dados_economicos(self, tabela):
        dados = self.gerenciador_bd.obter_dados(f"{tabela}_normalizados")
        if dados.empty:
            return None, None

        # Assumindo que queremos prever o pr√≥ximo valor com base nos √∫ltimos 30 dias
        janela_tempo = 30
        X, y = [], []
        for i in range(len(dados) - janela_tempo):
            X.append(dados['valor'].iloc[i:i+janela_tempo].values)
            y.append(dados['valor'].iloc[i+janela_tempo])

        return np.array(X), np.array(y)
    
    def preparar_dados_sentimento(self, tabela):
        dados = self.gerenciador_bd.obter_dados(f"{tabela}_normalizados")
        if dados.empty:
            return None, None

        # Assumindo que queremos prever o sentimento do pr√≥ximo dia com base nos √∫ltimos 7 dias
        janela_tempo = 7
        X, y = [], []
        for ticker in dados['ticker'].unique():
            dados_ticker = dados[dados['ticker'] == ticker].sort_values('data')
            for i in range(len(dados_ticker) - janela_tempo):
                X.append(dados_ticker['sentimento'].iloc[i:i+janela_tempo].values)
                y.append(dados_ticker['sentimento'].iloc[i+janela_tempo])

        return np.array(X), np.array(y)

    def prever(self, X, dias):
        if self.model is None:
            raise ValueError("Modelo n√£o treinado. Por favor, treine o modelo primeiro.")

        previsoes = []
        ultimo_valor_real = X[0, -1, 3]  # √öltimo valor de fechamento
        for _ in range(dias):
            previsao = self.model.predict(X)
            previsao_suavizada = 0.7 * previsao[0, 0] + 0.3 * ultimo_valor_real  # Fator de suaviza√ß√£o
            previsoes.append(previsao_suavizada)
            X = np.roll(X, -1, axis=1)
            X[0, -1, :] = previsao
            ultimo_valor_real = previsao_suavizada

        return np.array(previsoes)

    def treinar(self, tabela, epocas=100, batch_size=32):
        logger.debug(f"Iniciando treinamento para tabela: {tabela}")
        try:
            if tabela == 'dados_economicos':
                X, y = self.preparar_dados_economicos(tabela)
            elif tabela == 'sentimento_mercado':
                X, y = self.preparar_dados_sentimento(tabela)
            else:
                # C√≥digo existente para a√ß√µes e FIIs
                tickers_df = self.gerenciador_bd.executar_query(f"SELECT DISTINCT {'acao' if tabela == 'acoes' else 'fii'} FROM {tabela}_normalizados")
                tickers = tickers_df[tickers_df.columns[0]].tolist()
                logger.debug(f"Lista de tickers: {tickers}")

                X_all, y_all = [], []
                for ticker in tickers:
                    X, y = self.preparar_dados(tabela, ticker)
                    if X is not None and y is not None:
                        X_all.extend(X)
                        y_all.extend(y)
                X, y = np.array(X_all), np.array(y_all)

            if X is None or y is None or len(X) == 0 or len(y) == 0:
                logger.warning(f"N√£o h√° dados suficientes para treinar o modelo para {tabela}")
                return

            if self.model is None:
                input_dim = X.shape[2] if len(X.shape) > 2 else 1
                self.criar_modelo(input_dim)

            history = self.model.fit(
                X, y,
                epochs=epocas,
                batch_size=batch_size,
                validation_split=0.2,
                verbose=1
            )

            # Salvar o modelo treinado
            nome_modelo = f"modelo_{tabela}"
            self.salvar_modelo(nome_modelo)

            logger.debug("Treinamento conclu√≠do com sucesso e modelo salvo")
            return history
        except Exception as e:
            logger.error(f"Erro durante o treinamento: {str(e)}", exc_info=True)
            raise

    def avaliar(self, tabela):
        X, y = self.preparar_dados(tabela)
        
        if len(X) == 0 or len(y) == 0:
            print(f"N√£o h√° dados suficientes para avaliar o modelo para {tabela}")
            return
        
        if self.model is None:
            raise ValueError("Modelo n√£o treinado. Por favor, treine o modelo primeiro.")
        
        loss = self.model.evaluate(X, y)
        print(f'Erro Quadr√°tico M√©dio: {loss:.4f}')

    def validar_previsoes(self, previsoes, ultimos_valores_reais):
        variacao_maxima_diaria = max(ultimos_valores_reais) * 0.05  # 5% de varia√ß√£o m√°xima di√°ria
        ultimo_valor_real = ultimos_valores_reais[-1]
        previsoes_validadas = []
        for i, previsao in enumerate(previsoes):
            variacao = previsao - ultimo_valor_real
            if abs(variacao) > variacao_maxima_diaria:
                logger.warning(f"Previs√£o {i+1} ({previsao:.2f}) parece estar fora da faixa esperada.")
                # Ajustar a previs√£o para estar dentro da faixa aceit√°vel
                if variacao > 0:
                    previsao = ultimo_valor_real + variacao_maxima_diaria
                else:
                    previsao = ultimo_valor_real - variacao_maxima_diaria
            previsoes_validadas.append(previsao)
            ultimo_valor_real = previsao  # Atualiza o √∫ltimo valor para a pr√≥xima itera√ß√£o
        return previsoes_validadas

if __name__ == '__main__':
    treinador = TreinadorKAN(output_dim=1, num_splines=10)
    for tabela in ['acoes', 'fiis']:
        print(f'Treinando modelo para {tabela}:')
        treinador.treinar(tabela)
        print(f'Avaliando modelo para {tabela}:')
        treinador.avaliar(tabela)

File: app\modelo\__init__.py
------------------------

File: app\modelo\__pycache__\kan.cpython-39.pyc
------------------------
a

    -nÆfE  „                   @   s6   d dl ZG ddÑ dejjÉZG ddÑ dejjjÉZdS )È    Nc                       s2   e Zd Zdá fddÑ	ZddÑ Zá fddÑZá  ZS )	⁄KANÈ
   c                    s`   t t| É†°  tjjjd |fdç| _tjjj|ddç| _	ddÑ t
|ÉD É| _tjj†|°| _
d S )N)⁄shapeT)⁄return_sequencesc                 S   s   g | ]
}t É ëqS © )⁄SplineActivation)⁄.0⁄_r   r   ˙<C:\Users\welli\OneDrive\Documentos\finaiti\app\modelo\kan.py⁄
<listcomp>   Û    z KAN.__init__.<locals>.<listcomp>)⁄superr   ⁄__init__⁄tf⁄keras⁄layers⁄Input⁄input_layer⁄LSTM⁄
lstm_layer⁄range⁄spline_activations⁄Dense⁄output_layer)⁄self⁄	input_dim⁄
output_dim⁄num_splines©⁄	__class__r   r
   r      s
    zKAN.__init__c                    sH   | † |°â tjá fddÑt| jÉD Éddçâ | †à d d Ödd d Öf °S )Nc                    s*   g | ]"\}}|à d d Öd d Ö|f ÉëqS ©Nr   )r   ⁄i⁄
activation©⁄xr   r
   r   
   r   zKAN.call.<locals>.<listcomp>Èˇˇˇˇ)⁄axis)r   r   ⁄stack⁄	enumerater   r   ©r   ⁄inputsr   r#   r
   ⁄call   s    
"zKAN.callc                    s   t t| É†|° d| _d S )NT)r
   r   ⁄build⁄built©r   ⁄input_shaper   r   r
   r,      s    z	KAN.build)r   )⁄__name__⁄
__module__⁄__qualname__r   r+   r,   ⁄
__classcell__r   r   r   r
   r      s   r   c                       s6   e Zd Zdá fddÑ	ZddÑ ZddÑ Zd	d
Ñ Zá  ZS )r   È   È   c                    s   t É †°  || _|| _d S r    )r
   r   ⁄	num_knots⁄order)r   r6   r7   r   r   r
   r      s    
zSplineActivation.__init__c                 C   s>   | j d| jfdddç| _| j d| j| j d fdddç| _d S )N⁄knots⁄
random_normalT)⁄namer   ⁄initializer⁄	trainable⁄coeffsÈ   )⁄
add_weightr6   r8   r7   r=   r.   r   r   r
   r,      s    ¸¸zSplineActivation.buildc                 C   s
   | † |°S r    )⁄evaluate_spliner)   r   r   r
   r+   (   s    zSplineActivation.callc                 C   s`   t †|°}t| jÉD ]F}t †|| j| kt j°}|| j| | t †|| j|  | j	° 7 }q|S r    )
r   ⁄
zeros_liker   r6   ⁄castr8   ⁄float32r=   ⁄powr7   )r   r$   ⁄yr!   ⁄maskr   r   r
   r@   +   s
    
,z SplineActivation.evaluate_spline)r4   r5   )r0   r1   r2   r   r,   r+   r@   r3   r   r   r   r
   r      s   r   )⁄
tensorflowr   r   ⁄Modelr   r   ⁄Layerr   r   r   r   r
   ⁄<module>   s   

File: app\modelo\__pycache__\treinador.cpython-39.pyc
------------------------
a

    ãGØf+  „                   @   s  d dl Z d dlZd dlmZ e j†e j†e j†e°dd°°Z	ej†
d e	° d dlZd dlZ
d dlZd dlZd dlmZ d dlmZ d dlmZmZ d dlmZ d dlmZ G d	d
Ñ d
ÉZedkêredd
dçZdD ]8Z e!de õ dùÉ e†"e ° e!de õ dùÉ e†#e ° q“dS )È    N)⁄loggerz..)⁄
EarlyStopping)⁄
GerenciadorBD©⁄KAN⁄SplineActivation)⁄ColetorSentimento)⁄AnalisadorSentimentoc                   @   sj   e Zd ZdddÑZddÑ Zdd	d
ÑZddÑ Zd
dÑ ZddÑ ZddÑ Z	ddÑ Z
dddÑZddÑ ZddÑ Z
dS ) ⁄TreinadorKANÈ
   c                 C   sP   || _ || _d | _tjj†° | _tjj†	° | _
tÉ | _d | _
tÉ | _tÉ | _d S )N)⁄
output_dim⁄num_splines⁄model⁄tf⁄kerasZ
optimizers⁄Adam⁄	optimizer⁄lossesZMeanSquaredError⁄loss_fnr   ⁄gerenciador_bd⁄	input_dimr   ⁄coletor_sentimentor	   ⁄analisador_sentimento)⁄selfr   r
   © r   ˙BC:\Users\welli\OneDrive\Documentos\finaiti\app\modelo\treinador.py⁄__init__   s    zTreinadorKAN.__init__c                 C   sh   t †d|õ ù° || _t|| j| jÉ| _tjj	|dkr>d |fnddç}| †|° | jj
| j| jdç d S )NzCriando modelo com input_dim: È   )Nr   )⁄shape)r   ⁄loss)
r   ⁄debugr   r   r   r
   r   r   r   ⁄Input⁄compiler   r   )r   r   Zdummy_inputr   r   r   ⁄criar_modelo   s    
zTreinadorKAN.criar_modeloNÈ   r   c              
      s‘  |dkr| † |||°S êzpd|õ d|dkr.dndõ d|dkr@dndõ dù}| j†|d	|i°â à jrxt†d
|õ ù° W dS g d¢}à já fd
dÑ|D Édçâ à jD ]}|dkr†à | †d°à |< q†à †	° â t†
d|õ dà jõ ù° t†
dà jõ ù° t†
dà †° õ ù° g g  }}	t
tà É| | d ÉD ]J}
|†à j|
|
| Ö jdddçj° |	†à j|
| | d  d ° êq(t†|°t†|	°fW S  têyŒ } z,tjd|õ dt|Éõ ùddç W Y d }~dS d }~0 0 d S )N⁄dados_economicosz™
                SELECT DISTINCT ON (n.data) n.data, n.abertura, n.maxima, n.minima, n.fechamento, n.volume, COALESCE(s.sentimento, 0) as sentimento
                FROM zX_normalizados n
                LEFT JOIN sentimento_mercado s ON n.data = s.data AND n.⁄acoes⁄acao⁄fiiz$ = s.ticker
                WHERE n.zW = :ticker
                ORDER BY n.data DESC
                LIMIT 1000
            ⁄tickerzNenhum dado encontrado para ©NN)⁄idr'   r(   c                    s   g | ]}|à j v r|ëqS r   ©⁄columns)⁄.0⁄col©⁄dadosr   r   ⁄
<listcomp><   Û    z/TreinadorKAN.preparar_dados.<locals>.<listcomp>r,   ⁄data⁄float32zPreparando dados para ˙: zColunas dos dados: zPrimeiras linhas dos dados:
r   ©⁄axis⁄
fechamentozErro ao preparar dados para T©⁄exc_info)⁄preparar_dados_economicosr   ⁄executar_query⁄emptyr   ⁄warning⁄dropr-   ⁄astype⁄dropnar    r   ⁄head⁄range⁄len⁄append⁄iloc⁄values⁄np⁄array⁄	Exception⁄error⁄str)r   ⁄tabelar)   ⁄janela_tempoZhorizonte_previsao⁄queryZcolunas_para_removerr/   ⁄X⁄y⁄i⁄er   r0   r   ⁄preparar_dados'   s>    ˛˝¸

$$zTreinadorKAN.preparar_dadosc              
   C   sÆ   z:| j †° }| j †° }| j†|||° t†d|õ dù° W nn tjj	y® } zRdt
|Év rít†d° | j†°  | j†|||° t†d|õ dù° nÇ W Y d }~n
d }~0 0 d S )NzModelo z$ salvo com sucesso no banco de dadosu   n√£o existe a rela√ß√£ou0   Tabela modelos_treinados n√£o existe. Criando...u'    salvo com sucesso ap√≥s criar a tabela)
r   ⁄to_jsonZget_weightsr   ⁄
salvar_modelor   ⁄info⁄
sqlalchemy⁄exc⁄ProgrammingErrorrM   r?   ⁄criar_tabela_modelos)r   ⁄nome_modeloZmodelo_serializado⁄pesosrT   r   r   r   rW   U   s    



zTreinadorKAN.salvar_modeloc              
   C   s∏   d|õ ù}zf| j †|°\}}|r^|r^tjjj|ttdúdç| _| j†	|° t
†d|õ dù° nt
†d|õ ù° W nB t
y≤ } z*t
jd|õ dt|Éõ ùd	d
ç W Y d }~n
d }~0 0 d S )N⁄modelo_r   )Zcustom_objectszModelo para z carregado com sucessozNenhum modelo encontrado para zErro ao carregar modelo para r6   Tr:   )r   ⁄carregar_modelor   r   ⁄modelsZmodel_from_jsonr   r   r   ⁄set_weightsr   rX   r?   rK   rL   rM   )r   rN   r]   ⁄modelo_jsonr^   rT   r   r   r   r`   d   s    
zTreinadorKAN.carregar_modeloc                 C   sä   | j †|õ dù°}|jrdS d}g g  }}tt|É| ÉD ]:}|†|d j||| Ö j° |†|d j||  ° q:t†	|°t†	|°fS )N⁄
_normalizadosr*   r$   ⁄valor)
r   ⁄obter_dadosr>   rD   rE   rF   rG   rH   rI   rJ   )r   rN   r1   rO   rQ   rR   rS   r   r   r   r<   q   s    
z&TreinadorKAN.preparar_dados_economicosc           	      C   s≤   | j †|õ dù°}|jrdS d}g g  }}|d †° D ]f}||d |k †d°}tt|É| ÉD ]:}|†|d j||| Ö j	° |†|d j||  ° q`q6t
†|°t
†|°fS )Nrd   r*   È   r)   r4   ⁄
sentimento)r   rf   r>   ⁄unique⁄sort_valuesrD   rE   rF   rG   rH   rI   rJ   )	r   rN   r1   rO   rQ   rR   r)   Zdados_tickerrS   r   r   r   ⁄preparar_dados_sentimento   s    
z&TreinadorKAN.preparar_dados_sentimentoc                 C   sÜ   | j d u rtdÉÇg }|d }t|ÉD ]T}| j †|°}d|d  d|  }|†|° tj|dddç}||d	dd d Öf< |}q&t†|°S )
Nı:   Modelo n√£o treinado. Por favor, treine o modelo primeiro.)r   ÈˇˇˇˇÈ   gffffffÊ?)r   r   g333333”?rm   r   r7   r   )r   ⁄
ValueErrorrD   ⁄predictrF   rI   ⁄rollrJ   )r   rQ   ⁄dias⁄	previsoes⁄ultimo_valor_real⁄_⁄previsaoZprevisao_suavizadar   r   r   ⁄preverè   s    

zTreinadorKAN.preverÈd   È    c              
   C   s‰  t †d|õ ù° êzå|dkr,| †|°\}}næ|dkrD| †|°\}}n¶| j†d|dkrXdndõ d|õ d	ù°}||jd
  †° }t †d|õ ù° g g  }}	|D ]8}
| †||
°\}}|d urö|d urö|†	|° |	†	|° qöt
†|°t
†|	° }}|d u ês|d u êst|Éd
kêst|Éd
kêr0t †
d|õ ù° W d S | jd u êrdt|jÉd
kêrV|jd
 nd}| †|° | jj||||dddç}d|õ ù}
| †|
° t †d° |W S  têyﬁ } z&t jdt|Éõ ùddç Ç W Y d }~n
d }~0 0 d S )Nz#Iniciando treinamento para tabela: r%   ⁄sentimento_mercadozSELECT DISTINCT r&   r'   r(   z FROM rd   r   zLista de tickers: u6   N√£o h√° dados suficientes para treinar o modelo para È   r   göôôôôô…?)⁄epochs⁄
batch_sizeZvalidation_split⁄verboser_   u1   Treinamento conclu√≠do com sucesso e modelo salvozErro durante o treinamento: Tr:   )r   r    r<   rk   r   r=   r-   ⁄tolistrU   ⁄extendrI   rJ   rE   r?   r   r   r#   ⁄fitrW   rK   rL   rM   )r   rN   Zepocasr}   rQ   rR   Z
tickers_df⁄tickersZX_all⁄y_allr)   r   ⁄historyr]   rT   r   r   r   ⁄treinarü   sH    &

0
˚	


zTreinadorKAN.treinarc                 C   sl   | † |°\}}t|Édks&t|Édkr8td|õ ùÉ d S | jd u rJtdÉÇ| j†||°}td|dõùÉ d S )Nr   u6   N√£o h√° dados suficientes para avaliar o modelo para rl   u   Erro Quadr√°tico M√©dio: z.4f)rU   rE   ⁄printr   ro   ⁄evaluate)r   rN   rQ   rR   r   r   r   r   ⁄avaliarŒ   s    
zTreinadorKAN.avaliarc           	      C   sà   t |Éd }|d }g }t|ÉD ]b\}}|| }t|É|krtt†d|d õ d|dõdù° |dkrl|| }n|| }|†|° |}q |S )	Ngöôôôôô©?rm   u
   Previs√£o r   z (z.2fz&) parece estar fora da faixa esperada.r   )⁄max⁄	enumerate⁄absr   r?   rF   )	r   rs   ⁄ultimos_valores_reaisZvariacao_maxima_diariart   ⁄previsoes_validadasrS   rv   Zvariacaor   r   r   ⁄validar_previsoes€   s    

zTreinadorKAN.validar_previsoes)r   )Nr$   r   )rx   ry   )⁄__name__⁄
__module__⁄__qualname__r   r#   rU   rW   r`   r<   rk   rw   rÖ   rà   ré   r   r   r   r   r
      s   

.

/
r
   ⁄__main__r   r   )r   r
   )r&   ⁄fiiszTreinando modelo para ˙:zAvaliando modelo para )$⁄os⁄sys⁄venvr   ⁄path⁄abspath⁄join⁄dirname⁄__file__⁄project_root⁄insertrY   ⁄
tensorflowr   ⁄numpyrI   ⁄pandas⁄pdZkeras.callbacksr   ⁄banco_dados.gerenciador_bdr   Zapp.modelo.kanr   r   ⁄#app.coleta_dados.coletor_sentimentor   ⁄*app.preprocessamento.analisador_sentimentor	   r
   rè   ⁄	treinadorrN   rÜ   rÖ   rà   r   r   r   r   ⁄<module>   s,     Z



File: app\modelo\__pycache__\__init__.cpython-39.pyc
------------------------
a

    ]H¨f    „                   @   s   d S )N© r   r   r   ˙AC:\Users\welli\OneDrive\Documentos\finaiti\app\modelo\__init__.py⁄<module>   Û    

File: app\preprocessamento\analisador_sentimento.py
------------------------
from textblob import TextBlob
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import pandas as pd
import re

class AnalisadorSentimento:
    def __init__(self):
        self.tokenizer = AutoTokenizer.from_pretrained("neuralmind/bert-base-portuguese-cased")
        self.model = AutoModelForSequenceClassification.from_pretrained("neuralmind/bert-base-portuguese-cased")

    def analisar(self, texto):
        inputs = self.tokenizer(texto, return_tensors="pt", truncation=True, padding=True, max_length=512)
        outputs = self.model(**inputs)
        probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
        return probabilities[0].tolist()

    def limpar_texto(self, texto):
        texto = re.sub(r'http\S+', '', texto)  # remove URLs
        texto = re.sub(r'@\w+', '', texto)     # remove men√ß√µes
        texto = re.sub(r'#\w+', '', texto)     # remove hashtags
        return texto.strip()

    def analisar_sentimento(self, texto):
        texto_limpo = self.limpar_texto(texto)
        analise = TextBlob(texto_limpo)
        return analise.sentiment.polarity

    def analisar_tweets(self, df):
        df['sentimento'] = df['texto'].apply(self.analisar_sentimento)
        return df
    
# Fun√ß√£o para mapear as probabilidades para um valor de sentimento entre -1 e 1
def mapear_sentimento(probabilidades):
    return (probabilidades[1] - probabilidades[0]) * 2 - 1

File: app\preprocessamento\limpeza_dados.py
------------------------
import pandas as pd
import numpy as np
import os
import sys

# Adiciona o diret√≥rio raiz do projeto ao sys.path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.insert(0, project_root)

from banco_dados.gerenciador_bd import GerenciadorBD

class LimpadorDados:
    def __init__(self):
        self.gerenciador_bd = GerenciadorBD()

    def limpar_dados(self, tabela):
        try:
            dados = self.gerenciador_bd.obter_dados(tabela)
            if dados.empty:
                print(f"Aviso: N√£o h√° dados na tabela {tabela}")
                return pd.DataFrame()
            dados_limpos = self.remover_valores_ausentes(dados)
            dados_limpos = self.tratar_outliers(dados_limpos)
            return dados_limpos
        except Exception as e:
            print(f"Erro ao limpar dados da tabela {tabela}: {str(e)}")
            return pd.DataFrame()

    def remover_valores_ausentes(self, dados):
        return dados.dropna()

    def tratar_outliers(self, dados):
        for coluna in dados.select_dtypes(include=[np.number]).columns:
            q1 = dados[coluna].quantile(0.25)
            q3 = dados[coluna].quantile(0.75)
            iqr = q3 - q1
            limite_inferior = q1 - 1.5 * iqr
            limite_superior = q3 + 1.5 * iqr
            dados[coluna] = dados[coluna].clip(limite_inferior, limite_superior)
        return dados

if __name__ == '__main__':
    limpador = LimpadorDados()
    for tabela in ['acoes', 'fiis', 'dados_economicos']:
        dados_limpos = limpador.limpar_dados(tabela)
        if not dados_limpos.empty:
            print(f"Dados da tabela {tabela} limpos com sucesso.")
        else:
            print(f"N√£o foi poss√≠vel limpar os dados da tabela {tabela}.")

File: app\preprocessamento\normalizacao.py
------------------------
from venv import logger
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import os
import sys
import joblib

# Adiciona o diret√≥rio raiz do projeto ao sys.path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.insert(0, project_root)

from banco_dados.gerenciador_bd import GerenciadorBD

class Normalizador:
    def __init__(self):
        self.gerenciador_bd = GerenciadorBD()
        self.scalers = {}

    def obter_min_max(self, coluna, tabela):
        if coluna not in self.scalers:
            raise ValueError(f"Scaler n√£o encontrado para a coluna {coluna}")
        scaler = self.scalers[coluna]
        return scaler.data_min_[0], scaler.data_max_[0]

    def normalizar_dados(self, tabela):
        dados = self.gerenciador_bd.obter_dados(tabela)
        if dados.empty:
            logger.warning(f"N√£o h√° dados para normalizar na tabela {tabela}")
            return None
        
        # L√≥gica de normaliza√ß√£o espec√≠fica para cada tipo de dado
        if tabela == 'acoes' or tabela == 'fiis':
            colunas_para_normalizar = ['abertura', 'maxima', 'minima', 'fechamento', 'volume']
        elif tabela == 'dados_economicos':
            colunas_para_normalizar = ['valor']
        elif tabela == 'sentimento_mercado':
            colunas_para_normalizar = ['sentimento']
        else:
            logger.error(f"Tabela n√£o reconhecida: {tabela}")
            return None
        
        dados_normalizados = dados.copy()
        for coluna in colunas_para_normalizar:
            dados_normalizados[coluna] = self.scaler.fit_transform(dados[[coluna]])
        
        return dados_normalizados

    def salvar_dados_normalizados(self, tabela, dados_normalizados):
        if not dados_normalizados.empty:
            try:
                # Usar SQLAlchemy para salvar os dados
                dados_normalizados.to_sql(f'{tabela}_normalizados', self.gerenciador_bd.engine, if_exists='replace', index=False)
                print(f"Dados normalizados da tabela {tabela} salvos com sucesso.")
            except Exception as e:
                print(f"Erro ao salvar dados normalizados da tabela {tabela}: {str(e)}")
        else:
            print(f"N√£o h√° dados normalizados para salvar da tabela {tabela}.")

    def desnormalizar_valor(self, valor_normalizado, coluna, tabela):
        if coluna not in self.scalers:
            self.carregar_scalers(tabela)
        min_val, max_val = self.obter_min_max(coluna, tabela)
        if isinstance(valor_normalizado, np.ndarray):
            valor_normalizado = np.clip(valor_normalizado, 0, 1)
        else:
            valor_normalizado = max(0, min(valor_normalizado, 1))
        return valor_normalizado * (max_val - min_val) + min_val

    def salvar_scalers(self, tabela):
        scaler_path = os.path.join(project_root, 'data', f'{tabela}_scalers.joblib')
        os.makedirs(os.path.dirname(scaler_path), exist_ok=True)
        joblib.dump(self.scalers, scaler_path)

    def carregar_scalers(self, tabela):
        scaler_path = os.path.join(project_root, 'data', f'{tabela}_scalers.joblib')
        if os.path.exists(scaler_path):
            self.scalers = joblib.load(scaler_path)
        else:
            raise ValueError(f"Scalers para a tabela {tabela} n√£o encontrados. Execute a normaliza√ß√£o primeiro.")

if __name__ == '__main__':
    normalizador = Normalizador()
    for tabela in ['acoes', 'fiis', 'dados_economicos']:
        dados_normalizados = normalizador.normalizar_dados(tabela)
        normalizador.salvar_dados_normalizados(tabela, dados_normalizados)

File: app\preprocessamento\__init__.py
------------------------

File: app\preprocessamento\__pycache__\analisador_sentimento.cpython-39.pyc
------------------------
a

    íØf≈  „                   @   sN   d dl mZ d dlmZmZ d dlZd dlZd dlZG ddÑ dÉZ	ddÑ Z
dS )È    )⁄TextBlob)⁄
AutoTokenizer⁄"AutoModelForSequenceClassificationNc                   @   s4   e Zd ZddÑ ZddÑ ZddÑ ZddÑ Zd	d
Ñ ZdS )⁄AnalisadorSentimentoc                 C   s   t †d°| _t†d°| _d S )Nz%neuralmind/bert-base-portuguese-cased)r   Zfrom_pretrained⁄	tokenizerr   ⁄model)⁄self© r	   ˙XC:\Users\welli\OneDrive\Documentos\finaiti\app\preprocessamento\analisador_sentimento.py⁄__init__   s    zAnalisadorSentimento.__init__c                 C   sD   | j |dddddç}| jf i |§é}tjjj|jddç}|d †° S )N⁄ptTi   )Zreturn_tensorsZ
truncation⁄padding⁄
max_lengthÈˇˇˇˇ)⁄dimr   )r   r   ⁄torch⁄nn⁄
functional⁄softmax⁄logits⁄tolist)r   ⁄texto⁄inputs⁄outputsZ
probabilitiesr	   r	   r
   ⁄analisar   s    zAnalisadorSentimento.analisarc                 C   s2   t †dd|°}t †dd|°}t †dd|°}|†° S )Nzhttp\S+⁄ z@\w+z#\w+)⁄re⁄sub⁄strip)r   r   r	   r	   r
   ⁄limpar_texto   s    z!AnalisadorSentimento.limpar_textoc                 C   s   | † |°}t|É}|jjS )N)r   r   Z	sentimentZpolarity)r   r   Ztexto_limpoZanaliser	   r	   r
   ⁄analisar_sentimento   s    
z(AnalisadorSentimento.analisar_sentimentoc                 C   s   |d † | j°|d< |S )Nr   ⁄
sentimento)⁄applyr    )r   ⁄dfr	   r	   r
   ⁄analisar_tweets   s    z$AnalisadorSentimento.analisar_tweetsN)⁄__name__⁄
__module__⁄__qualname__r   r   r   r    r$   r	   r	   r	   r
   r      s
   r   c                 C   s   | d | d  d d S )NÈ   r   È   r	   )⁄probabilidadesr	   r	   r
   ⁄mapear_sentimento"   s    r+   )Ztextblobr   Ztransformersr   r   r   ⁄pandas⁄pdr   r   r+   r	   r	   r	   r
   ⁄<module>   s   

File: app\preprocessamento\__pycache__\normalizacao.cpython-39.pyc
------------------------
a

    ãGØfó  „                   @   s∫   d dl mZ d dlZd dlZd dlmZ d dlZd dl	Z	d dl
Z
ej†ej†
ej†e°dd°°Ze	j†d e° d dlmZ G ddÑ dÉZedkr∂eÉ Zd	D ]Ze†e°Ze†ee° qödS )
È    )⁄loggerN)⁄MinMaxScalerz..)⁄
GerenciadorBDc                   @   sD   e Zd ZddÑ ZddÑ ZddÑ ZddÑ Zd	d
Ñ ZddÑ Zd
dÑ Z	dS )⁄Normalizadorc                 C   s   t É | _i | _d S )N)r   ⁄gerenciador_bd⁄scalers)⁄self© r	   ˙OC:\Users\welli\OneDrive\Documentos\finaiti\app\preprocessamento\normalizacao.py⁄__init__   s    zNormalizador.__init__c                 C   s6   || j vrtd|õ ùÉÇ| j | }|jd |jd fS )Nu%   Scaler n√£o encontrado para a coluna r   )r   ⁄
ValueError⁄	data_min_⁄	data_max_)r   ⁄coluna⁄tabela⁄scalerr	   r	   r
   ⁄
obter_min_max   s    

zNormalizador.obter_min_maxc                 C   s†   | j †|°}|jr&t†d|õ ù° d S |dks6|dkr@g d¢}n4|dkrPdg}n$|dkr`dg}nt†d	|õ ù° d S |†° }|D ]}| j†||g °||< qÄ|S )
Nu)   N√£o h√° dados para normalizar na tabela ⁄acoes⁄fiis)⁄abertura⁄maxima⁄minima⁄
fechamento⁄volume⁄dados_economicos⁄valor⁄sentimento_mercado⁄
sentimentou   Tabela n√£o reconhecida: )	r   ⁄obter_dados⁄emptyr   ⁄warning⁄error⁄copyr   ⁄
fit_transform)r   r   ⁄dadosZcolunas_para_normalizar⁄dados_normalizadosr   r	   r	   r
   ⁄normalizar_dados   s     
zNormalizador.normalizar_dadosc              
   C   sä   |j svz0|j|õ dù| jjdddç td|õ dùÉ W qÜ tyr } z$td|õ dt|Éõ ùÉ W Y d }~qÜd }~0 0 ntd	|õ d
ùÉ d S )N⁄
_normalizados⁄replaceF)⁄	if_exists⁄indexzDados normalizados da tabela z salvos com sucesso.z,Erro ao salvar dados normalizados da tabela z: u2   N√£o h√° dados normalizados para salvar da tabela ⁄.)r   ⁄to_sqlr   ⁄engine⁄print⁄	Exception⁄str)r   r   r%   ⁄er	   r	   r
   ⁄salvar_dados_normalizados1   s    0z&Normalizador.salvar_dados_normalizadosc                 C   s`   || j vr| †|° | †||°\}}t|tjÉr@t†|dd°}ntdt|dÉÉ}|||  | S )Nr   È   )	r   ⁄carregar_scalersr   ⁄
isinstance⁄np⁄ndarray⁄clip⁄max⁄min)r   Zvalor_normalizador   r   ⁄min_val⁄max_valr	   r	   r
   ⁄desnormalizar_valor<   s    

z Normalizador.desnormalizar_valorc                 C   s>   t j†td|õ dù°}t jt j†|°ddç t†| j|° d S )N⁄data˙_scalers.joblibT)⁄exist_ok)	⁄os⁄path⁄join⁄project_root⁄makedirs⁄dirname⁄joblib⁄dumpr   ©r   r   Zscaler_pathr	   r	   r
   ⁄salvar_scalersF   s    zNormalizador.salvar_scalersc                 C   sD   t j†td|õ dù°}t j†|°r0t†|°| _ntd|õ dùÉÇd S )Nr>   r?   zScalers para a tabela u5    n√£o encontrados. Execute a normaliza√ß√£o primeiro.)	rA   rB   rC   rD   ⁄existsrG   ⁄loadr   r   rI   r	   r	   r
   r4   K   s    zNormalizador.carregar_scalersN)
⁄__name__⁄
__module__⁄__qualname__r   r   r&   r2   r=   rJ   r4   r	   r	   r	   r
   r      s   
r   ⁄__main__)r   r   r   )⁄venvr   ⁄numpyr6   ⁄pandas⁄pdZsklearn.preprocessingr   rA   ⁄sysrG   rB   ⁄abspathrC   rF   ⁄__file__rD   ⁄insert⁄banco_dados.gerenciador_bdr   r   rM   ⁄normalizadorr   r&   r%   r2   r	   r	   r	   r
   ⁄<module>   s    C


File: app\preprocessamento\__pycache__\__init__.cpython-39.pyc
------------------------
a

    ]H¨f    „                   @   s   d S )N© r   r   r   ˙KC:\Users\welli\OneDrive\Documentos\finaiti\app\preprocessamento\__init__.py⁄<module>   Û    

File: app\__pycache__\__init__.cpython-39.pyc
------------------------
a

    ]H¨f    „                   @   s   d S )N© r   r   r   ˙:C:\Users\welli\OneDrive\Documentos\finaiti\app\__init__.py⁄<module>   Û    

File: banco_dados\gerenciador_bd.py
------------------------
import os
import sys
import psycopg2
import pandas as pd
from psycopg2 import sql
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker
from sqlalchemy.pool import QueuePool
import logging
import subprocess
from sqlalchemy.engine.url import URL
import pickle

# Adicione o diret√≥rio raiz do projeto ao PYTHONPATH
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, project_root)

from config.configuracao import obter_config_bd

logger = logging.getLogger(__name__)

class GerenciadorBD:
    def __init__(self):
        url = URL.create(
            drivername="postgresql",
            username=os.getenv('DB_USER'),
            password=os.getenv('DB_PASSWORD'),
            host=os.getenv('DB_HOST'),
            port=os.getenv('DB_PORT'),
            database=os.getenv('DB_NAME')
        )
        self.engine = create_engine(url, connect_args={"connect_timeout": 5})
        self.Session = sessionmaker(bind=self.engine)
        logger.debug("Engine do banco de dados criado")

    def testar_conexao(self):
        try:
            with self.engine.connect() as conn:
                result = conn.execute(text("SELECT 1"))
                logger.info("Conex√£o com o banco de dados bem-sucedida!")
                return result.fetchone()[0] == 1
        except Exception as e:
            logger.error(f"Erro ao conectar ao banco de dados: {str(e)}")
            raise


    def conectar(self):
        return self.engine.connect()

    def executar_transacao(self, queries):
        logger.debug(f"Iniciando transa√ß√£o com {len(queries)} queries")
        try:
            with self.conectar() as conn:
                with conn.begin():
                    for query in queries:
                        conn.execute(query)
            logger.debug("Transa√ß√£o conclu√≠da com sucesso")
        except Exception as e:
            logger.error(f"Erro na transa√ß√£o: {str(e)}", exc_info=True)
            raise

    def criar_tabela_modelos(self):
        query = """
        CREATE TABLE IF NOT EXISTS modelos_treinados (
            id SERIAL PRIMARY KEY,
            nome VARCHAR(100) UNIQUE,
            modelo_json TEXT,
            pesos BYTEA,
            data_criacao TIMESTAMP
        )
        """
        self.executar_query(query)

    def criar_tabelas(self):
        with self.engine.connect() as conn:
            # Tabela acoes
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS acoes (
                    id SERIAL PRIMARY KEY,
                    acao VARCHAR(10),
                    data DATE,
                    abertura FLOAT,
                    maxima FLOAT,
                    minima FLOAT,
                    fechamento FLOAT,
                    volume FLOAT,
                    CONSTRAINT acoes_unique UNIQUE (acao, data)
                )
            """))

            # Tabela acoes_normalizados
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS acoes_normalizados (
                    id SERIAL PRIMARY KEY,
                    acao VARCHAR(10),
                    data DATE,
                    abertura FLOAT,
                    maxima FLOAT,
                    minima FLOAT,
                    fechamento FLOAT,
                    volume FLOAT,
                    UNIQUE (acao, data)
                )
            """))

            # Tabela dados_economicos
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS dados_economicos (
                    id SERIAL PRIMARY KEY,
                    indicador VARCHAR(50),
                    data DATE,
                    valor FLOAT,
                    UNIQUE (indicador, data)
                )
            """))

            # Tabela dados_economicos_normalizados
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS dados_economicos_normalizados (
                    id SERIAL PRIMARY KEY,
                    indicador VARCHAR(50),
                    data DATE,
                    valor FLOAT,
                    UNIQUE (indicador, data)
                )
            """))

            # Tabela fiis
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS fiis (
                    id SERIAL PRIMARY KEY,
                    fii VARCHAR(10),
                    data DATE,
                    abertura FLOAT,
                    maxima FLOAT,
                    minima FLOAT,
                    fechamento FLOAT,
                    volume FLOAT,
                    dividendos FLOAT,
                    UNIQUE (fii, data)
                )
            """))

            # Tabela fiis_normalizados
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS fiis_normalizados (
                    id SERIAL PRIMARY KEY,
                    fii VARCHAR(10),
                    data DATE,
                    abertura FLOAT,
                    maxima FLOAT,
                    minima FLOAT,
                    fechamento FLOAT,
                    volume FLOAT,
                    dividendos FLOAT,
                    UNIQUE (fii, data)
                )
            """))

            # Tabela sentimento_mercado
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS sentimento_mercado (
                    id SERIAL PRIMARY KEY,
                    data DATE NOT NULL,
                    ticker VARCHAR(20) NOT NULL,
                    sentimento FLOAT NOT NULL,
                    CONSTRAINT sentimento_mercado_unique UNIQUE (data, ticker)
                )
            """))

            conn.commit()
        
        logger.info("Todas as tabelas foram criadas ou j√° existem.")
        
    def criar_tabela_sentimento(self):
        self.criar_tabelas()  # Isso criar√° todas as tabelas, incluindo sentimento_mercado
        logger.info("Tabela sentimento_mercado criada ou j√° existente")

    def inserir_dados(self, tabela, dados):
        if not dados:
            logger.warning(f"N√£o h√° dados para inserir na tabela {tabela}")
            return

        try:
            dados_df = pd.DataFrame(dados)
            dados_df.to_sql(tabela, self.engine, if_exists='append', index=False)
            logger.info(f"Dados inseridos com sucesso na tabela {tabela}")
            
            # Verificar o conte√∫do da tabela ap√≥s a inser√ß√£o
            verificacao = self.executar_query(f"SELECT * FROM {tabela} LIMIT 5")
            logger.debug(f"Primeiras 5 linhas da tabela {tabela} ap√≥s inser√ß√£o:\n{verificacao}")
        except Exception as e:
            logger.error(f"Erro ao inserir dados na tabela {tabela}: {str(e)}", exc_info=True)
            raise

    def executar_query(self, query, params=None):
        logger.debug(f"Executando query: {query}")
        try:
            with self.engine.connect() as conn:
                if isinstance(params, list):
                    # Executar inser√ß√£o em lote
                    conn.execute(text(query), params)
                    conn.commit()
                    return None
                else:
                    result = conn.execute(text(query), params)
                    conn.commit()
                    if result.returns_rows:
                        return pd.DataFrame(result.fetchall(), columns=result.keys())
                    else:
                        return None
        except Exception as e:
            logger.error(f"Erro ao executar query: {str(e)}", exc_info=True)
            raise

    def atualizar_dados(self, tabela, condicao, novos_valores):
        try:
            set_clause = ', '.join([f"{k} = :{k}" for k in novos_valores.keys()])
            query = f"UPDATE {tabela} SET {set_clause} WHERE {condicao}"
            params = {**novos_valores, **dict([c.split('=') for c in condicao.split(' AND ')])}
            
            with self.engine.connect() as conn:
                conn.execute(text(query), params)
                conn.commit()
            logger.info(f"Dados atualizados com sucesso na tabela {tabela}")
        except Exception as e:
            logger.error(f"Erro ao atualizar dados na tabela {tabela}: {str(e)}", exc_info=True)
            raise

    def excluir_dados(self, tabela, condicao):
        comando = sql.SQL("DELETE FROM {} WHERE {}").format(
            sql.Identifier(tabela),
            sql.SQL(condicao)
        )
        
        with self.conectar() as conn:
            with conn.begin():
                conn.execute(comando)

    def obter_dados(self, tabela):
        comando = f"SELECT * FROM {tabela}"
        return self.executar_query(comando)

    def fazer_backup(self, caminho_backup):
        logger.info(f"Iniciando backup do banco de dados para {caminho_backup}")
        try:
            comando = f"pg_dump -h {self.config['host']} -U {self.config['user']} -d {self.config['database']} -f {caminho_backup}"
            subprocess.run(comando, shell=True, check=True)
            logger.info("Backup conclu√≠do com sucesso")
        except subprocess.CalledProcessError as e:
            logger.error(f"Erro ao fazer backup: {str(e)}", exc_info=True)
            raise

    def restaurar_backup(self, caminho_backup):
        logger.info(f"Iniciando restaura√ß√£o do banco de dados a partir de {caminho_backup}")
        try:
            comando = f"psql -h {self.config['host']} -U {self.config['user']} -d {self.config['database']} -f {caminho_backup}"
            subprocess.run(comando, shell=True, check=True)
            logger.info("Restaura√ß√£o conclu√≠da com sucesso")
        except subprocess.CalledProcessError as e:
            logger.error(f"Erro ao restaurar backup: {str(e)}", exc_info=True)
            raise

    def executar_migracao(self, caminho_script):
        logger.info(f"Executando script de migra√ß√£o: {caminho_script}")
        try:
            with open(caminho_script, 'r') as script:
                comandos = script.read()
            self.executar_transacao([comandos])
            logger.info("Migra√ß√£o conclu√≠da com sucesso")
        except Exception as e:
            logger.error(f"Erro ao executar migra√ß√£o: {str(e)}", exc_info=True)
            raise

    def verificar_tabela(self, tabela):
        try:
            query = text(f"SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = '{tabela}')")
            with self.engine.connect() as conn:
                result = conn.execute(query)
                exists = result.scalar()
            logger.info(f"Tabela {tabela} {'existe' if exists else 'n√£o existe'}")
            return exists
        except Exception as e:
            logger.error(f"Erro ao verificar tabela {tabela}: {str(e)}", exc_info=True)
            return False
        
    def salvar_modelo(self, nome_modelo, modelo_json, pesos):
        pesos_serializados = pickle.dumps(pesos)
        query = """
        INSERT INTO modelos_treinados (nome, modelo_json, pesos, data_criacao)
        VALUES (:nome, :modelo_json, :pesos, CURRENT_TIMESTAMP)
        ON CONFLICT (nome) DO UPDATE
        SET modelo_json = :modelo_json, pesos = :pesos, data_criacao = CURRENT_TIMESTAMP
        """
        self.executar_query(query, {'nome': nome_modelo, 'modelo_json': modelo_json, 'pesos': pesos_serializados})

    def carregar_modelo(self, nome_modelo):
        query = "SELECT modelo_json, pesos FROM modelos_treinados WHERE nome = :nome"
        resultado = self.executar_query(query, {'nome': nome_modelo})
        if not resultado.empty:
            modelo_json = resultado.iloc[0]['modelo_json']
            pesos = pickle.loads(resultado.iloc[0]['pesos'])
            return modelo_json, pesos
        return None, None
    
    def upsert_dados(self, tabela, dados, chaves_unicas):
        if not dados:
            logger.warning(f"N√£o h√° dados para inserir/atualizar na tabela {tabela}")
            return

        try:
            colunas = list(dados[0].keys())
            placeholders = ', '.join([f':{col}' for col in colunas])
            update_set = ', '.join([f"{col} = EXCLUDED.{col}" for col in colunas if col not in chaves_unicas])
            
            query = f"""
                INSERT INTO {tabela} ({', '.join(colunas)})
                VALUES ({placeholders})
                ON CONFLICT ({', '.join(chaves_unicas)}) DO UPDATE SET
                {update_set}
            """
            
            self.executar_query(query, dados)
            logger.info(f"Dados inseridos/atualizados com sucesso na tabela {tabela}")
        except Exception as e:
            logger.error(f"Erro ao inserir/atualizar dados na tabela {tabela}: {str(e)}")
            raise

    def criar_indices(self):
        indices = [
            ("acoes", "acao"),
            ("acoes", "data"),
            ("fiis", "fii"),
            ("fiis", "data"),
            ("dados_economicos", "indicador"),
            ("dados_economicos", "data"),
            ("sentimento_mercado", "ticker"),
            ("sentimento_mercado", "data")
        ]
        for tabela, coluna in indices:
            query = f"CREATE INDEX IF NOT EXISTS idx_{tabela}_{coluna} ON {tabela} ({coluna})"
            self.executar_query(query)

    def limpar_banco_dados(self):
        tabelas = ['acoes', 'acoes_normalizados', 'fiis', 'fiis_normalizados', 
                'dados_economicos', 'dados_economicos_normalizados', 
                'sentimento_mercado', 'modelos_treinados']
        
        try:
            with self.engine.connect() as conn:
                for tabela in tabelas:
                    conn.execute(text(f"TRUNCATE TABLE {tabela} RESTART IDENTITY CASCADE"))
                conn.commit()
            logger.info("Todas as tabelas foram limpas com sucesso.")
        except Exception as e:
            logger.error(f"Erro ao limpar o banco de dados: {str(e)}")
            raise

    def obter_ultima_atualizacao(self, tabela):
        query = f"SELECT MAX(data) FROM {tabela}"
        resultado = self.executar_query(query)
        return resultado.iloc[0][0] if not resultado.empty else None

    def obter_ultima_data(self, tabela, identificador=None):
        if tabela == 'acoes':
            coluna_id, coluna_data = 'acao', 'data'
        elif tabela == 'fiis':
            coluna_id, coluna_data = 'fii', 'data'
        elif tabela == 'dados_economicos':
            coluna_id, coluna_data = 'indicador', 'data'
        elif tabela == 'sentimento_mercado':
            coluna_id, coluna_data = 'ticker', 'data'
        else:
            raise ValueError(f"Tabela n√£o reconhecida: {tabela}")

        if identificador:
            query = f"SELECT MAX({coluna_data}) FROM {tabela} WHERE {coluna_id} = :identificador"
            params = {'identificador': identificador}
        else:
            query = f"SELECT MAX({coluna_data}) FROM {tabela}"
            params = {}

        resultado = self.executar_query(query, params)
        return resultado.iloc[0][0] if not resultado.empty and resultado.iloc[0][0] is not None else None
    
    def adicionar_restricao_unicidade(self, tabela, colunas):
        constraint_name = f"{tabela}_{'_'.join(colunas)}_unique"
        query = f"""
        DO $$
        BEGIN
            IF NOT EXISTS (
                SELECT 1
                FROM information_schema.table_constraints
                WHERE constraint_name = '{constraint_name}'
            ) THEN
                ALTER TABLE {tabela}
                ADD CONSTRAINT {constraint_name} UNIQUE ({', '.join(colunas)});
            END IF;
        END $$;
        """
        self.executar_query(query)
        logger.info(f"Restri√ß√£o de unicidade adicionada ou j√° existente para {tabela}: {colunas}")


if __name__ == '__main__':
    logging.basicConfig(level=logging.DEBUG)
    gerenciador = GerenciadorBD()
    gerenciador.criar_tabelas()
    gerenciador.criar_tabela_sentimento()

File: banco_dados\__init__.py
------------------------

File: banco_dados\__pycache__\gerenciador_bd.cpython-39.pyc
------------------------
a

    `NØf“?  „                   @   s¯   d dl Z d dlZd dlZd dlZd dlmZ d dlmZmZ d dl	m
Z
 d dlmZ d dl
Z
d dlZd dlmZ d dlZe j†e j†e j†e°d°°Zej†d e° d dlmZ e
†e°ZG d	d
Ñ d
ÉZedkrÙe
je
j dç eÉ Z!e!†"°  e!†#°  dS )
È    N)⁄sql)⁄
create_engine⁄text)⁄sessionmaker)⁄	QueuePool)⁄URLz..)⁄obter_config_bdc                   @   s–   e Zd ZddÑ ZddÑ ZddÑ ZddÑ Zd	d
Ñ ZddÑ Zd
dÑ Z	ddÑ Z
d2ddÑZddÑ ZddÑ Z
ddÑ ZddÑ ZddÑ ZddÑ Zd d!Ñ Zd"d#Ñ Zd$d%Ñ Zd&d'Ñ Zd(d)Ñ Zd*d+Ñ Zd,d-Ñ Zd3d.d/ÑZd0d1Ñ ZdS )4⁄
GerenciadorBDc              	   C   sb   t jdt†d°t†d°t†d°t†d°t†d°dç}t|dd	id
ç| _t| jdç| _t†	d° d S )
NZ
postgresql⁄DB_USER⁄DB_PASSWORD⁄DB_HOST⁄DB_PORT⁄DB_NAME)Z
drivername⁄username⁄password⁄host⁄port⁄database⁄connect_timeoutÈ   )Zconnect_args)⁄bindzEngine do banco de dados criado)
r   ⁄create⁄os⁄getenvr   ⁄enginer   ⁄Session⁄logger⁄debug)⁄self⁄url© r    ˙HC:\Users\welli\OneDrive\Documentos\finaiti\banco_dados\gerenciador_bd.py⁄__init__   s    ˙zGerenciadorBD.__init__c              
   C   sò   zX| j †° è:}|†tdÉ°}t†d° |†° d dkW  d   É W S 1 sL0    Y  W n: tyí } z"t†dt	|Éõ ù° Ç W Y d }~n
d }~0 0 d S )NzSELECT 1u+   Conex√£o com o banco de dados bem-sucedida!r   È   z$Erro ao conectar ao banco de dados: )
r   ⁄connect⁄executer   r   ⁄info⁄fetchone⁄	Exception⁄error⁄str)r   ⁄conn⁄result⁄er    r    r!   ⁄testar_conexao$   s    
4zGerenciadorBD.testar_conexaoc                 C   s
   | j †° S )N)r   r$   ©r   r    r    r!   ⁄conectar/   s    zGerenciadorBD.conectarc              
   C   sÃ   t †dt|Éõ dù° zr| †° èL}|†° è$ |D ]}|†|° q0W d   É n1 sT0    Y  W d   É n1 sr0    Y  t †d° W n> ty∆ } z&t jdt|Éõ ùddç Ç W Y d }~n
d }~0 0 d S )Nu   Iniciando transa√ß√£o com z queriesu"   Transa√ß√£o conclu√≠da com sucessou   Erro na transa√ß√£o: T©⁄exc_info)	r   r   ⁄lenr0   ⁄beginr%   r(   r)   r*   )r   Zqueriesr+   ⁄queryr-   r    r    r!   ⁄executar_transacao2   s    

Hz GerenciadorBD.executar_transacaoc                 C   s   d}| † |° d S )NzÌ
        CREATE TABLE IF NOT EXISTS modelos_treinados (
            id SERIAL PRIMARY KEY,
            nome VARCHAR(100) UNIQUE,
            modelo_json TEXT,
            pesos BYTEA,
            data_criacao TIMESTAMP
        )
        ©⁄executar_query)r   r5   r    r    r!   ⁄criar_tabela_modelos>   s    	z"GerenciadorBD.criar_tabela_modelosc                 C   s¢   | j †° èz}|†tdÉ° |†tdÉ° |†tdÉ° |†tdÉ° |†tdÉ° |†tdÉ° |†tdÉ° |†°  W d   É n1 sä0    Y  t†d° d S )	Na≤  
                CREATE TABLE IF NOT EXISTS acoes (
                    id SERIAL PRIMARY KEY,
                    acao VARCHAR(10),
                    data DATE,
                    abertura FLOAT,
                    maxima FLOAT,
                    minima FLOAT,
                    fechamento FLOAT,
                    volume FLOAT,
                    CONSTRAINT acoes_unique UNIQUE (acao, data)
                )
            aß  
                CREATE TABLE IF NOT EXISTS acoes_normalizados (
                    id SERIAL PRIMARY KEY,
                    acao VARCHAR(10),
                    data DATE,
                    abertura FLOAT,
                    maxima FLOAT,
                    minima FLOAT,
                    fechamento FLOAT,
                    volume FLOAT,
                    UNIQUE (acao, data)
                )
            a   
                CREATE TABLE IF NOT EXISTS dados_economicos (
                    id SERIAL PRIMARY KEY,
                    indicador VARCHAR(50),
                    data DATE,
                    valor FLOAT,
                    UNIQUE (indicador, data)
                )
            a-  
                CREATE TABLE IF NOT EXISTS dados_economicos_normalizados (
                    id SERIAL PRIMARY KEY,
                    indicador VARCHAR(50),
                    data DATE,
                    valor FLOAT,
                    UNIQUE (indicador, data)
                )
            aΩ  
                CREATE TABLE IF NOT EXISTS fiis (
                    id SERIAL PRIMARY KEY,
                    fii VARCHAR(10),
                    data DATE,
                    abertura FLOAT,
                    maxima FLOAT,
                    minima FLOAT,
                    fechamento FLOAT,
                    volume FLOAT,
                    dividendos FLOAT,
                    UNIQUE (fii, data)
                )
            a   
                CREATE TABLE IF NOT EXISTS fiis_normalizados (
                    id SERIAL PRIMARY KEY,
                    fii VARCHAR(10),
                    data DATE,
                    abertura FLOAT,
                    maxima FLOAT,
                    minima FLOAT,
                    fechamento FLOAT,
                    volume FLOAT,
                    dividendos FLOAT,
                    UNIQUE (fii, data)
                )
            aa  
                CREATE TABLE IF NOT EXISTS sentimento_mercado (
                    id SERIAL PRIMARY KEY,
                    data DATE NOT NULL,
                    ticker VARCHAR(20) NOT NULL,
                    sentimento FLOAT NOT NULL,
                    CONSTRAINT sentimento_mercado_unique UNIQUE (data, ticker)
                )
            u.   Todas as tabelas foram criadas ou j√° existem.)r   r$   r%   r   ⁄commitr   r&   )r   r+   r    r    r!   ⁄
criar_tabelasJ   s    
&zGerenciadorBD.criar_tabelasc                 C   s   | † °  t†d° d S )Nu1   Tabela sentimento_mercado criada ou j√° existente)r;   r   r&   r/   r    r    r!   ⁄criar_tabela_sentimentoØ   s    z%GerenciadorBD.criar_tabela_sentimentoc              
   C   sº   |st †d|õ ù° d S zZt†|°}|j|| jdddç t †d|õ ù° | †d|õ dù°}t †d|õ d	|õ ù° W nD t	y∂ } z,t j
d
|õ dt|Éõ ùdd
ç Ç W Y d }~n
d }~0 0 d S )Nu&   N√£o h√° dados para inserir na tabela ⁄appendF)⁄	if_exists⁄indexz&Dados inseridos com sucesso na tabela ˙SELECT * FROM z LIMIT 5zPrimeiras 5 linhas da tabela u    ap√≥s inser√ß√£o:
z Erro ao inserir dados na tabela ˙: Tr1   )r   ⁄warning⁄pd⁄	DataFrame⁄to_sqlr   r&   r8   r   r(   r)   r*   )r   ⁄tabela⁄dadosZdados_dfZverificacaor-   r    r    r!   ⁄
inserir_dados≥   s    
zGerenciadorBD.inserir_dadosNc              
   C   s  t †d|õ ù° z∏| j†° èö}t|tÉrR|†t|É|° |†°  W d   É W d S |†t|É|°}|†°  |j	rñt
j|†° |†
° dçW  d   É W S W d   É W d S W d   É n1 sº0    Y  W n@ têy } z&t jdt|Éõ ùddç Ç W Y d }~n
d }~0 0 d S )NzExecutando query: )⁄columnszErro ao executar query: Tr1   )r   r   r   r$   ⁄
isinstance⁄listr%   r   r:   Zreturns_rowsrC   rD   ⁄fetchall⁄keysr(   r)   r*   )r   r5   ⁄paramsr+   r,   r-   r    r    r!   r8   ƒ   s    
&4zGerenciadorBD.executar_queryc           	   
   C   sÓ   z§d† ddÑ |†° D É°}d|õ d|õ d|õ ù}i |•tddÑ |†d°D ÉÉ•}| j†° è(}|†t|É|° |†°  W d   É n1 sà0    Y  t	†
d	|õ ù° W nD tyË } z,t	jd
|õ dt
|Éõ ùdd
ç Ç W Y d }~n
d }~0 0 d S )N˙, c                 S   s   g | ]}|õ d |õ ùëqS )z = :r    )⁄.0⁄kr    r    r!   ⁄
<listcomp>⁄   Û    z1GerenciadorBD.atualizar_dados.<locals>.<listcomp>zUPDATE z SET ˙ WHERE c                 S   s   g | ]}|† d °ëqS )˙=)⁄split)rP   ⁄cr    r    r!   rR   ‹   rS   z AND z(Dados atualizados com sucesso na tabela z"Erro ao atualizar dados na tabela rA   Tr1   )⁄joinrM   ⁄dictrV   r   r$   r%   r   r:   r   r&   r(   r)   r*   )	r   rF   ⁄condicaoZ
novos_valoresZ
set_clauser5   rN   r+   r-   r    r    r!   ⁄atualizar_dadosÿ   s     &zGerenciadorBD.atualizar_dadosc              	   C   s|   t †d°†t †|°t †|°°}| †° èB}|†° è |†|° W d   É n1 sP0    Y  W d   É n1 sn0    Y  d S )NzDELETE FROM {} WHERE {})r   ZSQL⁄format⁄
Identifierr0   r4   r%   )r   rF   rZ   ⁄comandor+   r    r    r!   ⁄
excluir_dadosÊ   s    
˛

zGerenciadorBD.excluir_dadosc                 C   s   d|õ ù}| † |°S )Nr@   r7   )r   rF   r^   r    r    r!   ⁄obter_dados   s    
zGerenciadorBD.obter_dadosc              
   C   s¢   t †d|õ ù° zLd| jd õ d| jd õ d| jd õ d|õ ù}tj|d	d	d
ç t †d° W n@ tjyú } z&t jdt|Éõ ùd	d
ç Ç W Y d }~n
d }~0 0 d S )Nz(Iniciando backup do banco de dados para zpg_dump -h r   ˙ -U ⁄user˙ -d r   ˙ -f T©⁄shell⁄checku   Backup conclu√≠do com sucessozErro ao fazer backup: r1   ©r   r&   ⁄config⁄
subprocess⁄run⁄CalledProcessErrorr)   r*   ©r   Zcaminho_backupr^   r-   r    r    r!   ⁄fazer_backupÙ   s    .zGerenciadorBD.fazer_backupc              
   C   s¢   t †d|õ ù° zLd| jd õ d| jd õ d| jd õ d|õ ù}tj|d	d	d
ç t †d° W n@ tjyú } z&t jdt|Éõ ùd	d
ç Ç W Y d }~n
d }~0 0 d S )Nu6   Iniciando restaura√ß√£o do banco de dados a partir de zpsql -h r   ra   rb   rc   r   rd   Tre   u$   Restaura√ß√£o conclu√≠da com sucessozErro ao restaurar backup: r1   rh   rm   r    r    r!   ⁄restaurar_backup˛   s    .zGerenciadorBD.restaurar_backupc              
   C   s†   t †d|õ ù° zLt|dÉè}|†° }W d   É n1 s:0    Y  | †|g° t †d° W n> työ } z&t jdt|Éõ ùddç Ç W Y d }~n
d }~0 0 d S )Nu!   Executando script de migra√ß√£o: ⁄ru!   Migra√ß√£o conclu√≠da com sucessou   Erro ao executar migra√ß√£o: Tr1   )r   r&   ⁄open⁄readr6   r(   r)   r*   )r   Zcaminho_script⁄scriptZcomandosr-   r    r    r!   ⁄executar_migracao  s    &zGerenciadorBD.executar_migracaoc              
   C   s∫   zpt d|õ dùÉ}| j†° è"}|†|°}|†° }W d   É n1 sD0    Y  t†d|õ d|rbdndõ ù° |W S  ty¥ } z,tjd|õ dt	|Éõ ùd	d
ç W Y d }~dS d }~0 0 d S )NzISELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'z')zTabela ˙ Zexisteu   n√£o existezErro ao verificar tabela rA   Tr1   F)
r   r   r$   r%   ⁄scalarr   r&   r(   r)   r*   )r   rF   r5   r+   r,   ⁄existsr-   r    r    r!   ⁄verificar_tabela  s    
&zGerenciadorBD.verificar_tabelac                 C   s&   t †|°}d}| †||||dú° d S )Na  
        INSERT INTO modelos_treinados (nome, modelo_json, pesos, data_criacao)
        VALUES (:nome, :modelo_json, :pesos, CURRENT_TIMESTAMP)
        ON CONFLICT (nome) DO UPDATE
        SET modelo_json = :modelo_json, pesos = :pesos, data_criacao = CURRENT_TIMESTAMP
        )⁄nome⁄modelo_json⁄pesos)⁄pickle⁄dumpsr8   )r   ⁄nome_modelorz   r{   Zpesos_serializadosr5   r    r    r!   ⁄
salvar_modelo  s    
zGerenciadorBD.salvar_modeloc                 C   sH   d}| † |d|i°}|jsD|jd d }t†|jd d °}||fS dS )NzCSELECT modelo_json, pesos FROM modelos_treinados WHERE nome = :nomery   r   rz   r{   )NN)r8   ⁄empty⁄ilocr|   ⁄loads)r   r~   r5   ⁄	resultadorz   r{   r    r    r!   ⁄carregar_modelo)  s    zGerenciadorBD.carregar_modeloc           	         sÍ   |st †d|õ ù° d S zåt|d †° É}d†ddÑ |D É°}d†á fddÑ|D É°}d|õ dd†|°õ d	|õ d
d†à °õ d|õ dù}| †||° t †d
|õ ù° W n@ ty‰ } z(t †d|õ dt	|Éõ ù° Ç W Y d }~n
d }~0 0 d S )Nu0   N√£o h√° dados para inserir/atualizar na tabela r   rO   c                 S   s   g | ]}d |õ ùëqS )˙:r    ©rP   ⁄colr    r    r!   rR   9  rS   z.GerenciadorBD.upsert_dados.<locals>.<listcomp>c                    s"   g | ]}|à vr|õ d |õ ùëqS )z = EXCLUDED.r    rÜ   ©⁄
chaves_unicasr    r!   rR   :  rS   z
                INSERT INTO ˙ (z)
                VALUES (z)
                ON CONFLICT (z ) DO UPDATE SET
                z
            z2Dados inseridos/atualizados com sucesso na tabela z*Erro ao inserir/atualizar dados na tabela rA   )
r   rB   rK   rM   rX   r8   r&   r(   r)   r*   )	r   rF   rG   râ   ⁄colunasZplaceholdersZ
update_setr5   r-   r    rà   r!   ⁄upsert_dados2  s.    ˇˇ˛˝¸zGerenciadorBD.upsert_dadosc              
   C   sB   g d¢}|D ]0\}}d|õ d|õ d|õ d|õ dù	}| † |° qd S )N))⁄acoes⁄acao)rç   ⁄data)⁄fiis⁄fii)rê   rè   )⁄dados_economicos⁄	indicador)rí   rè   )⁄sentimento_mercado⁄ticker)rî   rè   zCREATE INDEX IF NOT EXISTS idx_⁄_z ON rä   ˙)r7   )r   ⁄indicesrF   Zcolunar5   r    r    r!   ⁄
criar_indicesI  s    
zGerenciadorBD.criar_indicesc              
   C   s®   g d¢}z`| j †° è8}|D ]}|†td|õ dùÉ° q|†°  W d   É n1 sR0    Y  t†d° W n: ty¢ } z"t†dt	|Éõ ù° Ç W Y d }~n
d }~0 0 d S )N)rç   Zacoes_normalizadosrê   Zfiis_normalizadosrí   Zdados_economicos_normalizadosrî   Zmodelos_treinadoszTRUNCATE TABLE z RESTART IDENTITY CASCADEz*Todas as tabelas foram limpas com sucesso.z!Erro ao limpar o banco de dados: )
r   r$   r%   r   r:   r   r&   r(   r)   r*   )r   Ztabelasr+   rF   r-   r    r    r!   ⁄limpar_banco_dadosX  s    &z GerenciadorBD.limpar_banco_dadosc                 C   s,   d|õ ù}| † |°}|js(|jd d S d S )NzSELECT MAX(data) FROM r   )r8   rÄ   rÅ   )r   rF   r5   rÉ   r    r    r!   ⁄obter_ultima_atualizacaog  s    

z&GerenciadorBD.obter_ultima_atualizacaoc                 C   s∆   |dkrd\}}nD|dkr$d\}}n2|dkr6d\}}n |dkrHd\}}nt d	|õ ùÉÇ|r|d
|õ d|õ d|õ d
ù}d|i}nd
|õ d|õ ù}i }| †||°}|js¬|jd d d ur¬|jd d S d S )Nrç   )ré   rè   rê   )rë   rè   rí   )rì   rè   rî   )rï   rè   u   Tabela n√£o reconhecida: zSELECT MAX(z) FROM rT   z = :identificador⁄
identificadorr   )⁄
ValueErrorr8   rÄ   rÅ   )r   rF   rú   Z	coluna_idZcoluna_datar5   rN   rÉ   r    r    r!   ⁄obter_ultima_datal  s     




zGerenciadorBD.obter_ultima_datac              
   C   s^   |õ dd† |°õ dù}d|õ d|õ d|õ dd† |°õ dù	}| †|° t†d	|õ d
|õ ù° d S )Nrñ   ⁄_uniquezµ
        DO $$
        BEGIN
            IF NOT EXISTS (
                SELECT 1
                FROM information_schema.table_constraints
                WHERE constraint_name = 'z1'
            ) THEN
                ALTER TABLE z 
                ADD CONSTRAINT z	 UNIQUE (rO   z/);
            END IF;
        END $$;
        u:   Restri√ß√£o de unicidade adicionada ou j√° existente para rA   )rX   r8   r   r&   )r   rF   rã   Zconstraint_namer5   r    r    r!   ⁄adicionar_restricao_unicidadeÇ  s    ˙¯	˜	˜
z+GerenciadorBD.adicionar_restricao_unicidade)N)N)⁄__name__⁄
__module__⁄__qualname__r"   r.   r0   r6   r9   r;   r<   rH   r8   r[   r_   r`   rn   ro   rt   rx   r   rÑ   rå   rô   rö   rõ   rû   r†   r    r    r    r!   r	      s0   
e




	
r	   ⁄__main__)⁄level)$r   ⁄sys⁄psycopg2⁄pandasrC   r   ⁄
sqlalchemyr   r   Zsqlalchemy.ormr   Zsqlalchemy.poolr   ⁄loggingrj   Zsqlalchemy.engine.urlr   r|   ⁄path⁄abspathrX   ⁄dirname⁄__file__Zproject_root⁄insertZconfig.configuracaor   ⁄	getLoggerr°   r   r	   ⁄basicConfig⁄DEBUGZgerenciadorr;   r<   r    r    r    r!   ⁄<module>   s0   
   

File: banco_dados\__pycache__\__init__.cpython-39.pyc
------------------------
a

    ]H¨f    „                   @   s   d S )N© r   r   r   ˙BC:\Users\welli\OneDrive\Documentos\finaiti\banco_dados\__init__.py⁄<module>   Û    

File: config\configuracao.py
------------------------
import os
from dotenv import load_dotenv

load_dotenv()

def obter_config_bd():
    return {
        'host': os.getenv('DB_HOST'),
        'port': os.getenv('DB_PORT'),
        'database': os.getenv('DB_NAME'),
        'user': os.getenv('DB_USER'),
        'password': os.getenv('DB_PASSWORD')
    }

File: config\__init__.py
------------------------

File: config\__pycache__\configuracao.cpython-39.pyc
------------------------
a

    Ú@¨f7  „                   @   s&   d dl Z d dlmZ eÉ  ddÑ ZdS )È    N)⁄load_dotenvc                   C   s.   t †d°t †d°t †d°t †d°t †d°dúS )N⁄DB_HOST⁄DB_PORT⁄DB_NAME⁄DB_USER⁄DB_PASSWORD)⁄host⁄port⁄database⁄user⁄password)⁄os⁄getenv© r   r   ˙AC:\Users\welli\OneDrive\Documentos\finaiti\config\configuracao.py⁄obter_config_bd   s    ˚r   )r
   ⁄dotenvr   r   r   r   r   r   ⁄<module>   s   

File: config\__pycache__\__init__.cpython-39.pyc
------------------------
a

    ]H¨f    „                   @   s   d S )N© r   r   r   ˙=C:\Users\welli\OneDrive\Documentos\finaiti\config\__init__.py⁄<module>   Û    

File: data\acoes_scalers.joblib
------------------------
ÄïN      }î(åaberturaîåsklearn.preprocessing._dataîåMinMaxScalerîìî)Åî}î(å
feature_rangeîK KÜîåcopyîàåclipîâåfeature_names_in_îåjoblib.numpy_pickleîåNumpyArrayWrapperîìî)Åî}î(åsubclassîånumpyîåndarrayîìîåshapeîKÖîåorderîåCîådtypeîhådtypeîìîåO8îâàáîRî(Kå|îNNNJˇˇˇˇJˇˇˇˇK?tîbå
allow_mmapîâånumpy_array_alignment_bytesîKubÄcnumpy.core.multiarray
_reconstruct
q cnumpy
ndarray
qK Öqc_codecs
encode
qX   bqX   latin1qÜqRqáqRq	(KKÖq
cnumpy
dtype
qX   O8qâàáq
Rq(KX   |qNNNJˇˇˇˇJˇˇˇˇK?tqbâ]qX   aberturaqatqb.ïx       ån_features_in_îKån_samples_seen_îM@ºåscale_îh)Åî}î(hhhKÖîhhhhåf8îâàáîRî(Kå<îNNNJˇˇˇˇJˇˇˇˇK tîbh!àh"KubˇˇˇˇˇcÏúöÇíã?ï*       åmin_îh)Åî}î(hhhKÖîhhhh+h!àh"Kubˇˇˇˇ        ï/       å	data_min_îh)Åî}î(hhhKÖîhhhh+h!àh"Kubˇˇˇˇˇˇˇˇˇˇˇˇˇˇˇ        ï/       å	data_max_îh)Åî}î(hhhKÖîhhhh+h!àh"Kubˇˇˇˇˇˇˇˇˇˇˇˇˇˇˇ6ıûl¡ëR@ï1       ådata_range_îh)Åî}î(hhhKÖîhhhh+h!àh"Kub
ˇˇˇˇˇˇˇˇˇˇˇˇˇ6ıûl¡ëR@ï]       å_sklearn_versionîå1.5.0îubåmaximaîh)Åî}î(hhh	àh
âhh)Åî}î(hhhKÖîhhhhh!âh"KubÄcnumpy.core.multiarray
_reconstruct
q cnumpy
ndarray
qK Öqc_codecs
encode
qX   bqX   latin1qÜqRqáqRq	(KKÖq
cnumpy
dtype
qX   O8qâàáq
Rq(KX   |qNNNJˇˇˇˇJˇˇˇˇK?tqbâ]qX   maximaqatqb.ï.       h#Kh$M@ºh%h)Åî}î(hhhKÖîhhhh+h!àh"Kubˇˇˇˇˇˇˇˇˇˇˇˇõ©>YZ\ã?ï%       h.h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ        ï%       h2h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ        ï%       h6h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ–œÊÇ∂R@ï%       h:h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ–œÊÇ∂R@ïF       h>h?ubåminimaîh)Åî}î(hhh	àh
âhh)Åî}î(hhhKÖîhhhhh!âh"KubÄcnumpy.core.multiarray
_reconstruct
q cnumpy
ndarray
qK Öqc_codecs
encode
qX   bqX   latin1qÜqRqáqRq	(KKÖq
cnumpy
dtype
qX   O8qâàáq
Rq(KX   |qNNNJˇˇˇˇJˇˇˇˇK?tqbâ]qX   minimaqatqb.ï.       h#Kh$M@ºh%h)Åî}î(hhhKÖîhhhh+h!àh"Kubˇˇˇ±ıÈ[e◊ã?ï%       h.h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ        ï%       h2h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ        ï%       h6h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ(~≥îœcR@ï%       h:h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ(~≥îœcR@ïJ       h>h?ubå
fechamentoîh)Åî}î(hhh	àh
âhh)Åî}î(hhhKÖîhhhhh!âh"KubÄcnumpy.core.multiarray
_reconstruct
q cnumpy
ndarray
qK Öqc_codecs
encode
qX   bqX   latin1qÜqRqáqRq	(KKÖq
cnumpy
dtype
qX   O8qâàáq
Rq(KX   |qNNNJˇˇˇˇJˇˇˇˇK?tqbâ]qX
   fechamentoqatqb.ï.       h#Kh$M@ºh%h)Åî}î(hhhKÖîhhhh+h!àh"Kubˇˇˇˇˇˇˇˇˇˇˇ	fÒ7W—î?ï%       h.h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇÍò7l2‡øï%       h2h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ   @≠Â8@ï%       h6h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ    ÅÖR@ï%       h:h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ   `+òH@ïF       h>h?ubåvolumeîh)Åî}î(hhh	àh
âhh)Åî}î(hhhKÖîhhhhh!âh"KubÄcnumpy.core.multiarray
_reconstruct
q cnumpy
ndarray
qK Öqc_codecs
encode
qX   bqX   latin1qÜqRqáqRq	(KKÖq
cnumpy
dtype
qX   O8qâàáq
Rq(KX   |qNNNJˇˇˇˇJˇˇˇˇK?tqbâ]qX   volumeqatqb.ï.       h#Kh$M@ºh%h)Åî}î(hhhKÖîhhhh+h!àh"KubˇˇˇY^¢Ωœ§2>ï%       h.h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ        ï%       h2h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ        ï%       h6h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ   Tv´Aï%       h:h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ   Tv´Aïl       h>h?ubå
dividendosîh)Åî}î(hhh	àh
âhh)Åî}î(hhhKÖîhhhhåO8îâàáîRî(KhNNNJˇˇˇˇJˇˇˇˇK?tîbh!âh"KubÄcnumpy.core.multiarray
_reconstruct
q cnumpy
ndarray
qK Öqc_codecs
encode
qX   bqX   latin1qÜqRqáqRq	(KKÖq
cnumpy
dtype
qX   O8qâàáq
Rq(KX   |qNNNJˇˇˇˇJˇˇˇˇK?tqbâ]qX
   dividendosqatqb.ïP       h#Kh$Móh%h)Åî}î(hhhKÖîhhhhåf8îâàáîRî(Kh,NNNJˇˇˇˇJˇˇˇˇK tîbh!àh"Kubˇˇˇˇˇˇˇ]t—EÌ?ï%       h.h)Åî}î(hhhKÖîhhhh£h!àh"Kub	ˇˇˇˇˇˇˇˇˇ        ï%       h2h)Åî}î(hhhKÖîhhhh£h!àh"Kub	ˇˇˇˇˇˇˇˇˇ        ï%       h6h)Åî}î(hhhKÖîhhhh£h!àh"Kub	ˇˇˇˇˇˇˇˇˇöôôôôôÒ?ï%       h:h)Åî}î(hhhKÖîhhhh£h!àh"Kub	ˇˇˇˇˇˇˇˇˇöôôôôôÒ?ï       h>h?ubu.

File: data\dados_economicos_scalers.joblib
------------------------
ÄïJ      }îåvalorîåsklearn.preprocessing._dataîåMinMaxScalerîìî)Åî}î(å
feature_rangeîK KÜîåcopyîàåclipîâåfeature_names_in_îåjoblib.numpy_pickleîåNumpyArrayWrapperîìî)Åî}î(åsubclassîånumpyîåndarrayîìîåshapeîKÖîåorderîåCîådtypeîhådtypeîìîåO8îâàáîRî(Kå|îNNNJˇˇˇˇJˇˇˇˇK?tîbå
allow_mmapîâånumpy_array_alignment_bytesîKubÄcnumpy.core.multiarray
_reconstruct
q cnumpy
ndarray
qK Öqc_codecs
encode
qX   bqX   latin1qÜqRqáqRq	(KKÖq
cnumpy
dtype
qX   O8qâàáq
Rq(KX   |qNNNJˇˇˇˇJˇˇˇˇK?tqbâ]qX   valorqatqb.ïx       ån_features_in_îKån_samples_seen_îMÚùåscale_îh)Åî}î(hhhKÖîhhhhåf8îâàáîRî(Kå<îNNNJˇˇˇˇJˇˇˇˇK tîbh!àh"Kubˇˇˇˇˇˇˇˇˇˇˇˇ7∞vE.±>ï*       åmin_îh)Åî}î(hhhKÖîhhhh+h!àh"Kubˇˇˇˇï@â&¶]ß>ï/       å	data_min_îh)Åî}î(hhhKÖîhhhh+h!àh"Kubˇˇˇˇˇˇˇˇˇˇˇˇˇˇˇ√ı(\è¬Âøï/       å	data_max_îh)Åî}î(hhhKÖîhhhh+h!àh"KubˇˇˇˇˇˇˇˇˇˇˇˇˇˇˇÕÃÃÃˇÃ-Aï1       ådata_range_îh)Åî}î(hhhKÖîhhhh+h!àh"Kub
ˇˇˇˇˇˇˇˇˇˇˇˇˇê¬ı(Õ-Aï       å_sklearn_versionîå1.5.0îubs.

File: data\fiis_scalers.joblib
------------------------
ÄïN      }î(åaberturaîåsklearn.preprocessing._dataîåMinMaxScalerîìî)Åî}î(å
feature_rangeîK KÜîåcopyîàåclipîâåfeature_names_in_îåjoblib.numpy_pickleîåNumpyArrayWrapperîìî)Åî}î(åsubclassîånumpyîåndarrayîìîåshapeîKÖîåorderîåCîådtypeîhådtypeîìîåO8îâàáîRî(Kå|îNNNJˇˇˇˇJˇˇˇˇK?tîbå
allow_mmapîâånumpy_array_alignment_bytesîKubÄcnumpy.core.multiarray
_reconstruct
q cnumpy
ndarray
qK Öqc_codecs
encode
qX   bqX   latin1qÜqRqáqRq	(KKÖq
cnumpy
dtype
qX   O8qâàáq
Rq(KX   |qNNNJˇˇˇˇJˇˇˇˇK?tqbâ]qX   aberturaqatqb.ïx       ån_features_in_îKån_samples_seen_îMóåscale_îh)Åî}î(hhhKÖîhhhhåf8îâàáîRî(Kå<îNNNJˇˇˇˇJˇˇˇˇK tîbh!àh"Kubˇˇˇˇˇø¿2L™x?ï*       åmin_îh)Åî}î(hhhKÖîhhhh+h!àh"Kubˇˇˇˇ        ï/       å	data_min_îh)Åî}î(hhhKÖîhhhh+h!àh"Kubˇˇˇˇˇˇˇˇˇˇˇˇˇˇˇ        ï/       å	data_max_îh)Åî}î(hhhKÖîhhhh+h!àh"Kubˇˇˇˇˇˇˇˇˇˇˇˇˇˇˇ∫n
¬d@ï1       ådata_range_îh)Åî}î(hhhKÖîhhhh+h!àh"Kub
ˇˇˇˇˇˇˇˇˇˇˇˇˇ∫n
¬d@ï]       å_sklearn_versionîå1.5.0îubåmaximaîh)Åî}î(hhh	àh
âhh)Åî}î(hhhKÖîhhhhh!âh"KubÄcnumpy.core.multiarray
_reconstruct
q cnumpy
ndarray
qK Öqc_codecs
encode
qX   bqX   latin1qÜqRqáqRq	(KKÖq
cnumpy
dtype
qX   O8qâàáq
Rq(KX   |qNNNJˇˇˇˇJˇˇˇˇK?tqbâ]qX   maximaqatqb.ï.       h#Kh$Móh%h)Åî}î(hhhKÖîhhhh+h!àh"Kubˇˇˇˇˇˇˇˇˇˇˇˇ˚zÜaø§x?ï%       h.h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ        ï%       h2h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ        ï%       h6h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ=¸È∑∆d@ï%       h:h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ=¸È∑∆d@ïF       h>h?ubåminimaîh)Åî}î(hhh	àh
âhh)Åî}î(hhhKÖîhhhhh!âh"KubÄcnumpy.core.multiarray
_reconstruct
q cnumpy
ndarray
qK Öqc_codecs
encode
qX   bqX   latin1qÜqRqáqRq	(KKÖq
cnumpy
dtype
qX   O8qâàáq
Rq(KX   |qNNNJˇˇˇˇJˇˇˇˇK?tqbâ]qX   minimaqatqb.ï.       h#Kh$Móh%h)Åî}î(hhhKÖîhhhh+h!àh"Kubˇˇˇì™Ç…Œx?ï%       h.h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ        ï%       h2h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ        ï%       h6h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇã˜HÅ£d@ï%       h:h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇã˜HÅ£d@ïJ       h>h?ubå
fechamentoîh)Åî}î(hhh	àh
âhh)Åî}î(hhhKÖîhhhhh!âh"KubÄcnumpy.core.multiarray
_reconstruct
q cnumpy
ndarray
qK Öqc_codecs
encode
qX   bqX   latin1qÜqRqáqRq	(KKÖq
cnumpy
dtype
qX   O8qâàáq
Rq(KX   |qNNNJˇˇˇˇJˇˇˇˇK?tqbâ]qX
   fechamentoqatqb.ï.       h#Kh$Móh%h)Åî}î(hhhKÖîhhhh+h!àh"Kubˇˇˇˇˇˇˇˇˇˇˇ
SÑËyz?ï%       h.h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇi
∫ÚÆøï%       h2h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ   Äôp"@ï%       h6h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ   †
¬d@ï%       h:h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ   õc@ïF       h>h?ubåvolumeîh)Åî}î(hhh	àh
âhh)Åî}î(hhhKÖîhhhhh!âh"KubÄcnumpy.core.multiarray
_reconstruct
q cnumpy
ndarray
qK Öqc_codecs
encode
qX   bqX   latin1qÜqRqáqRq	(KKÖq
cnumpy
dtype
qX   O8qâàáq
Rq(KX   |qNNNJˇˇˇˇJˇˇˇˇK?tqbâ]qX   volumeqatqb.ï.       h#Kh$Móh%h)Åî}î(hhhKÖîhhhh+h!àh"KubˇˇˇSFW⁄ø´~>ï%       h.h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ        ï%       h2h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ        ï%       h6h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ   `±`Aï%       h:h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ   `±`AïJ       h>h?ubå
dividendosîh)Åî}î(hhh	àh
âhh)Åî}î(hhhKÖîhhhhh!âh"KubÄcnumpy.core.multiarray
_reconstruct
q cnumpy
ndarray
qK Öqc_codecs
encode
qX   bqX   latin1qÜqRqáqRq	(KKÖq
cnumpy
dtype
qX   O8qâàáq
Rq(KX   |qNNNJˇˇˇˇJˇˇˇˇK?tqbâ]qX
   dividendosqatqb.ï.       h#Kh$Móh%h)Åî}î(hhhKÖîhhhh+h!àh"Kubˇˇˇˇˇˇˇˇˇˇˇ]t—EÌ?ï%       h.h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ        ï%       h2h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇ        ï%       h6h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇöôôôôôÒ?ï%       h:h)Åî}î(hhhKÖîhhhh+h!àh"Kub	ˇˇˇˇˇˇˇˇˇöôôôôôÒ?ï       h>h?ubu.

File: templates\index.html
------------------------
<!DOCTYPE html>
<html lang="pt-br">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Previs√£o de A√ß√µes e FIIs - FinAIti</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/moment"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-adapter-moment"></script>
</head>
<body>
    <div class="container mt-5">
        <h1 class="mb-4">Previs√£o de A√ß√µes e FIIs - FinAIti</h1>
        
        <div class="row">
            <div class="col-md-6">
                <h2>Treinar Modelo</h2>
                <form id="trainForm" class="mb-4">
                    <div class="mb-3">
                        <label for="tabela" class="form-label">Tabela:</label>
                        <select id="tabela" name="tabela" class="form-select">
                            <option value="acoes">A√ß√µes</option>
                            <option value="fiis">FIIs</option>
                            <option value="dados_economicos">Dados Econ√¥micos</option>
                        </select>
                    </div>
                    <button type="submit" class="btn btn-primary" id="trainButton">Treinar Modelo</button>
                </form>
                <div id="message" class="alert" style="display: none;"></div>
            </div>
            
            <div class="col-md-6">
                <h2>Fazer Previs√£o</h2>
                <form id="predictForm" class="mb-4">
                    <div class="mb-3">
                        <label for="predictTabela" class="form-label">Tabela:</label>
                        <select id="predictTabela" name="predictTabela" class="form-select">
                            <option value="acoes">A√ß√µes</option>
                            <option value="fiis">FIIs</option>
                        </select>
                    </div>
                    <div id="tickerCheckboxes" class="mb-3">
                        <!-- Checkboxes ser√£o adicionados aqui dinamicamente -->
                    </div>
                    <button type="submit" class="btn btn-success" id="predictButton">Fazer Previs√£o</button>
                </form>
            </div>
        </div>
        
        <div id="resultado" class="mt-4" style="display: none;">
            <h3>Resultado da Previs√£o</h3>
            <p id="previsaoTexto"></p>
            <canvas id="previsaoGrafico" width="400" height="200"></canvas>
        </div>
    </div>
    <div id="resultado" class="mt-4" style="display: none;">
        <h3>Resultado da Previs√£o</h3>
        <p id="previsaoTexto"></p>
        <p id="sentimentoTexto"></p>
        <canvas id="previsaoGrafico" width="400" height="200"></canvas>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
    <script>
        console.log("Chart.js carregado:", typeof Chart !== 'undefined');

        const predictTabela = document.getElementById('predictTabela');
        const tickerCheckboxes = document.getElementById('tickerCheckboxes');
        const acoes = JSON.parse('{{ acoes|tojson|safe }}');
        const fiis = JSON.parse('{{ fiis|tojson|safe }}');

        function createCheckboxes(lista) {
            tickerCheckboxes.innerHTML = '';
            lista.forEach(item => {
                const div = document.createElement('div');
                div.className = 'form-check';
                const input = document.createElement('input');
                input.type = 'checkbox';
                input.className = 'form-check-input';
                input.id = item;
                input.name = 'tickers';
                input.value = item;
                const label = document.createElement('label');
                label.className = 'form-check-label';
                label.htmlFor = item;
                label.textContent = item;
                div.appendChild(input);
                div.appendChild(label);
                tickerCheckboxes.appendChild(div);
            });
        }

        predictTabela.addEventListener('change', function() {
            const lista = this.value === 'acoes' ? acoes : fiis;
            createCheckboxes(lista);
        });

        // Inicializar checkboxes com a√ß√µes
        createCheckboxes(acoes);

        document.getElementById('trainForm').addEventListener('submit', function(e) {
            e.preventDefault();
            const tabela = document.getElementById('tabela').value;
            const trainButton = document.getElementById('trainButton');
            trainButton.disabled = true;
            trainButton.textContent = 'Treinando...';

            fetch('/treinar', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/x-www-form-urlencoded',
                },
                body: `tabela=${tabela}`
            })
            .then(response => response.json())
            .then(data => {
                const messageElement = document.getElementById('message');
                messageElement.textContent = data.message || data.error;
                messageElement.style.display = 'block';
                messageElement.className = data.error ? 'alert alert-danger' : 'alert alert-success';
            })
            .catch(error => {
                console.error('Erro:', error);
                const messageElement = document.getElementById('message');
                messageElement.textContent = 'Ocorreu um erro ao treinar o modelo.';
                messageElement.style.display = 'block';
                messageElement.className = 'alert alert-danger';
            })
            .finally(() => {
                trainButton.disabled = false;
                trainButton.textContent = 'Treinar Modelo';
            });
        });

        document.getElementById('predictForm').addEventListener('submit', function(e) {
            e.preventDefault();
            const tabela = document.getElementById('predictTabela').value;
            const tickers = Array.from(document.querySelectorAll('input[name="tickers"]:checked')).map(el => el.value);
            const predictButton = document.getElementById('predictButton');
            predictButton.disabled = true;
            predictButton.textContent = 'Prevendo...';

            console.log("Enviando requisi√ß√£o de previs√£o...");

            const params = new URLSearchParams();
            params.append('tabela', tabela);
            tickers.forEach(ticker => params.append('tickers', ticker));

            fetch(`/prever?${params.toString()}`)
            .then(response => {
                console.log("Resposta recebida");
                return response.json();
            })
            .then(data => {
                console.log("Dados recebidos:", data);
                if (data.erro) {
                    alert(data.erro);
                } else {
                    document.getElementById('resultado').style.display = 'block';
                    document.getElementById('previsaoTexto').textContent = `Previs√£o realizada para ${tickers.join(', ')}`;
                    console.log("Chamando renderizarGrafico");
                    renderizarGrafico(data);
                }
            })
            .catch(error => {
                console.error('Erro:', error);
                alert('Ocorreu um erro ao fazer a previs√£o.');
            })
            .finally(() => {
                predictButton.disabled = false;
                predictButton.textContent = 'Fazer Previs√£o';
            });
        });

        function renderizarGrafico(data) {
            console.log("Dados recebidos para o gr√°fico:", data);
            
            const ctx = document.getElementById('previsaoGrafico').getContext('2d');
            
            if (window.previsaoChart) {
                window.previsaoChart.destroy();
            }

            const datasets = [];
            const colors = ['rgb(75, 192, 192)', 'rgb(255, 99, 132)', 'rgb(54, 162, 235)', 'rgb(255, 206, 86)', 'rgb(153, 102, 255)'];

            Object.keys(data).forEach((ticker, index) => {
                const tickerData = data[ticker];
                datasets.push({
                    label: `${ticker} - Hist√≥rico`,
                    data: tickerData.ultimos_valores_reais.map((valor, i) => ({x: tickerData.datas_historicas[i], y: valor})),
                    borderColor: colors[index % colors.length],
                    tension: 0.1,
                    pointRadius: 0
                });
                datasets.push({
                    label: `${ticker} - Previs√£o`,
                    data: tickerData.previsao.map((valor, i) => ({x: tickerData.datas_futuras[i], y: valor})),
                    borderColor: colors[index % colors.length],
                    borderDash: [5, 5],
                    tension: 0.1,
                    pointRadius: 0
                });

                document.getElementById('sentimentoTexto').textContent += `Sentimento m√©dio para ${ticker}: ${tickerData.sentimento_medio.toFixed(2)} `;
            });

            window.previsaoChart = new Chart(ctx, {
                type: 'line',
                data: {
                    datasets: datasets
                },
                options: {
                    responsive: true,
                    scales: {
                        x: {
                            type: 'time',
                            time: {
                                unit: 'day',
                                parser: 'yyyy-MM-dd',
                                tooltipFormat: 'll'
                            },
                            title: {
                                display: true,
                                text: 'Data'
                            }
                        },
                        y: {
                            beginAtZero: false,
                            title: {
                                display: true,
                                text: 'Valor'
                            }
                        }
                    },
                    plugins: {
                        tooltip: {
                            mode: 'index',
                            intersect: false
                        },
                        legend: {
                            position: 'top',
                        }
                    }
                }
            });
        }
    </script>
</body>
</html>

File: testes\__init__.py
------------------------

File: web\__init__.py
------------------------

